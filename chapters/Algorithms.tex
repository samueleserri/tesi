\chapter{Algorithms for NMF}
In this chapter, some algorithms are derived and presented.\
\par
Recall that the NMF problem \ref{NMF optimization problem} is NP-hard \cite{vavasis2007complexitynonnegativematrixfactorization}, hence all the Algorithms presented in this chapter are heuristics that aim to find a local minimum of the optimization problem.
More precisely, they are iterative schemes that start from an initial guess of the factors \( W \) and \( H \) and iteratively update them to reduce the value of the objective function, converging to a local minimum.
They don't guarantee to converge to the global minimum.
\section{Two blocks coordinate descent (2-BCD)}
The most common setting for NMF is the so called two-blocks coordinate descent, in this setting the factors \( W \) and \( H \) are updated alternately, fixing one while updating the other.

\begin{algorithm}[h]\label{2-BCD}
    \caption{Two-blocks coordinate descent for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \(W^{(k)}\) $\gets$ Update (\( W^{(k - 1)} \), \( H^{(k-1)} \))
            \Comment{Update \( W \) by fixing \( H \)}
            \State \(H^{(k)}\) $\gets$ Update (\( H^{(k - 1)} \), \( W^{(k)} \))
            \Comment{Update \( H \) by fixing \( W \)}
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
This general scheme is widely used, and we will present different ways to update the factors \( W \) and \( H \). There are mainly two observations that justify this approach:
\begin{enumerate}
    \item When one of the factors is fixed, the optimization subproblem is convex for most of the common cost functions. That makes it easier to design algorithms to solve them.
    \item Due to the symmetry of the problem $V = WH \iff V^T = H^T W^T$ the value of the cost function is the same if we swap \( W \) and \( H \) and transpose the input matrix \( V \). $D(V, WH) = D(V^T, H^T W^T)$.
    Therefore, it is sufficient to design an update rule for one of the factors, the other can be obtained by swapping the roles of \( W \) and \( H \) and transposing \( V \).
\end{enumerate}
Overall this approach is much easier to handle than trying to update both factors simultaneously. See \cite{mukkamala2019alternatingupdatesmatrixfactorization}.
\subsection{Exact 2-BCD}
If the subproblems of \ref{2-BCD} are solved exactly at each iteration, the method is called exact 2-BCD. 
And takes the following form:
\begin{algorithm}[H]\label{Exact 2-BCD}
    \caption{Exact two-blocks coordinate descent for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k)} \leftarrow \arg\min_{W \geq 0} D(V, WH^{(k - 1)}) \)
            \State \( H^{(k)} \leftarrow \arg\min_{H \geq 0} D(V, W^{(k)}H) \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
In this case we have the following convergence result:
\begin{theorem}
    Let $\min_{W,H \geq 0} D(V, WH)$ be the NMF optimization problem, and
    \( \{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}} \) be the sequence generated by the exact 2-BCD method.
    if \begin{enumerate}
        \item The function \( D \) is continuously differentiable.
        \item Each block of variables $W$ and $H$ belong to a closed convex set. \label{Condition 2 Exact 2-BCD}
    \end{enumerate}
    Then the limit points of the sequence \( \{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}} \) are stationary points.
\end{theorem}
\begin{proof}
    this theorem is a corollary (Corollary 2) of the main theorem presented in \cite{GRIPPO2000127}.
\end{proof}
\begin{observation}
    In the NMF problem, the condition \ref{Condition 2 Exact 2-BCD} is always satisfied because the elements of $W$ and $H$ are constrained to lie in the non negative orthant.
\end{observation}
\section{Stopping criteria}
All the algorithms presented in this chapter are iterative methods that require a stopping criterion to terminate.
Usually, the implementation of these algorithms include a maximum number of iterations after which the algorithm stops.
However, others stopping criteria can be used in addition to the maximum number of iterations.
\par
A common stopping criterion is to check the relative error at each step and stop the algorithm if no significant improvement is observed for a given number of steps $T$.
For instance, we can use the following rule:
\begin{equation}
    |e(t - T) - e(t)| \leq tol\times e(t)
\end{equation}
Where $e(t)$ is the relative error at iteration $t$. This is equal to:
\begin{equation}  \label{stopping criterion I}
    e(t) = \frac{\|V - W^{(t)}H^{(t)}\|_F^2}{\|V\|_F^2}
\end{equation}
in the case of the Frobenius norm. 
$tol$ is a small tolerance value,  for instance $10^{-4}$, and $T$ is the lag used to compare the current error with the error $T$ iterates before.
\par
Another effective stopping criterion is to check the norm of the difference between the iterates and stop if it falls below a certain threshold.
\begin{align}
    &\|W^{(t)} - W^{(t-T)}\|_F^2 \leq tol \times \|W^{(t - T)}\|_F^2 \\
    &\|H^{(t)} - H^{(t-T)}\|_F^2 \leq tol \times \|H^{(t - T)}\|_F^2
\end{align}
Again a small value for $tol$, for instance $10^{-4}$, and $T = 10$ are reasonable choices in practice.
\par
The complete algorithm with the stopping criterion \ref{stopping criterion I} is summarized in the following pseudo-code:
\begin{algorithm}[H]\label{2-BCD}
    \caption{Two-blocks coordinate descent for NMF with stopping criteria \ref{stopping criterion I}}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \), tolerance \( tol \), lag \( T \), maximum iterations \( \text{max\_iter} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \State Compute \( e(0) = \frac{\|V - W^{(0)}H^{(0)}\|_F^2}{\|V\|_F^2} \)
        \Comment{Initial relative error}
        \For {\( k = 0, 1, 2, \ldots, \text{max\_iter} \)}
            \State \(W^{(k)}\) $\gets$ Update (\( W^{(k - 1)} \), \( H^{(k-1)} \))
            \Comment{Update \( W \) by fixing \( H \)}
            \State \(H^{(k)}\) $\gets$ Update (\( H^{(k - 1)} \), \( W^{(k)} \))
            \Comment{Update \( H \) by fixing \( W \)}
            \State Compute \( e(k) = \frac{\|V - W^{(k)}H^{(k)}\|_F^2}{\|V\|_F^2} \)
            \Comment{Compute relative error}
            \If {\( k  \geq T \) \textbf{and} \( |e(k - T) - e(k)| \leq tol \times e(k) \)}
                \State \textbf{break}
                \Comment{If stopping criterion met, exit loop}
            \EndIf
        \EndFor
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\section{Initialization}
TODO
\section{Multiplicative Updates (MU)} \label{MU derivation section}
The multiplicative updates (MU) were first introduced by Lee and Seung in \cite{Lee1999}. They are one of the most popular algorithms for NMF.
In this section we will derive the MU algorithm for two different cost functions: the Frobenius norm and the Kullback-Leibler divergence.
The Lee and Seung paper with converge proof is \cite{lee2000algorithms}. A derivation using matrix algebra can be found in \cite{burred2014detailed}.
In section \ref{MU beta section} we will present the MU algorithm for the more general case of beta divergence \cite{févotte2011algorithmsnonnegativematrixfactorization}.
\subsection{MU with the Frobenius norm}
In this section we will derive the MU iteration scheme for the case where
$D(V, WH) = \|V - WH\|_F^2$ is the Frobenius norm.
The scheme can be interpreted as a scaled gradient descent method.\\
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top (WH - V) = W^\top WH - W^\top V
\end{equation}
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top WH} \circ \nabla_H \frac{1}{2} \|V - WH\|_F^2
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top V}{W^\top WH}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W \frac{1}{2} \|V - WH\|_F^2 = (WH - V) H^\top = W H H^\top - V H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{W H H^\top} \circ \nabla_W \frac{1}{2} \|V - WH\|_F^2
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{V H^\top}{W H H^\top}
\end{equation}
\par
Recall that in section \ref{Gradient descent section} we have introduce the gradient descent as an iterative method for finding local minima with the following update rule: $x_{k+1} = x_k - \eta_k\nabla f(x_k)$.
The re-scaled gradient descent used in the MU updates can be interpreted as a gradient descent with a variable step size $\eta_k$ that is constrained to ensure the non-negativity of the iterates.
We derive the MU updates for $W$:
\begin{equation} \label{GD MU updates}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2
\end{equation}
To ensure the non-negativity of the iterates, we impose the following condition:
\begin{equation}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2 \geq 0
\end{equation}
This leads to the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2} = \frac{W^{(k)}}{W^{(k)} H H^\top - V H^\top}
\end{equation}
Since $VH^\top \geq 0$ and $W^{(k)} H H^\top \geq 0$, we can write the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{W^{(k)} H H^\top} < \frac{W^{(k)}}{W^{(k)} H H^\top - V H^\top}
\end{equation}
The selecting the step size as:
\begin{equation}
    \eta_k = \frac{W^{(k)}}{W^{(k)} H H^\top}
\end{equation}
and substituting in equation \ref{GD MU updates} we obtain the MU update for \( W \):
\begin{equation}
    W^{(k+1)} = W^{(k)} - \frac{W^{(k)}}{W^{(k)} H H^\top} \circ (W^{(k)} H H^\top - V H^\top)
\end{equation}
And simplifying we obtain the update rule for \( W \):
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{V H^\top}{W^{(k)} H H^\top}}
\end{equation}
Analogously, we can derive the MU update for \( H \) and using the gradient $\nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top WH - W^\top V$ we obtain:
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^\top V}{W^\top W H^{(k)}}}
\end{equation}
\par The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[H]\label{MU for Frobenius}
    \caption{Multiplicative Updates for NMF with the Frobenius norm}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} V}{W^{(k) \top} W^{(k)} H^{(k)}} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{V H^{(k+1) \top}}{W^{(k)} H^{(k+1)} H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the Frobenius norm}
\begin{theorem}[\cite{lee2000algorithms}, Theorem 1] \label{MU convergence Fro}
    The value of the objective function $D(V, WH) = \|V - WH \|_F^2$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by Multiplicative updates \ref{MU for Frobenius}.
    Furthermore $W^{(k)}$ and $H^{(k)}$ are stationary points of the objective $D(V, WH)$ if and only if the distance is invariant under the iterative scheme \ref{MU for Frobenius}.
\end{theorem}
\begin{proof}
    TODO
\end{proof}
\subsubsection{Computational cost}
The computational cost of the MU algorithm with the Frobenius norm depends on the computation of the gradients, $WHH^\top$ and $VH^\top$ for the update of $W$.
And $W^\top WH$ and $W^\top V$ for the update of $H$.
Recall that the dimensions of the matrices are:
\begin{itemize}
    \item \( V \in \mathbb{R}_+^{m \times n} \)
    \item \( W \in \mathbb{R}_+^{m \times r} \)
    \item \( H \in \mathbb{R}_+^{r \times n} \)
\end{itemize}
When computing $WHH^\top$ one should pay attention to compute $W(HH^\top)$ instead of $(WH)H^\top$ to reduce the computational cost.
The cost of the latter is $O(mnr + mnr^2)$ while the cost of the former is $O(mr^2 + r^2 n) = O(r^2(m+n))$. And in most NMF applications $r \ll \min(m,n)$.
Similarly, when computing $W^\top WH$ one should compute $(W^\top W)H$ instead of $W^\top (WH)$ to reduce the computational cost. Again we have that the former requires $O(mr^2 + r^2 n) = O(r^2(m+n))$ operations while the latter requires $O(mnr + mnr^2)$ operations.
\subsection{MU with the Kullback-Leibler divergence}
In this section we will derive the MU iteration scheme for the case where
$D(V, WH) = D_{KL}(V \| WH)$ is the Kullback-Leibler divergence. 
Again this scheme can be interpreted as a scaled gradient descent method, in fact more generally the MU updates can be interpreted as a scaled gradient descent method for a wide class of cost functions.
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H D_{KL}(V \| WH) = W^\top \left(\textbf{1} - \frac{V}{WH} \right) = W^\top \textbf{1} - W^\top \frac{V}{WH}
\end{equation}
Where $\textbf{1}$ is the matrix of all ones with the same dimensions as \( V \).
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top \textbf{1}} \circ \nabla_H D_{KL}(V \| WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top \frac{V}{WH}}{W^\top \textbf{1}}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W D_{KL}(V \| WH) = \left(\textbf{1} - \frac{V}{WH} \right) H^\top = \textbf{1} H^\top - \frac{V}{WH} H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{\textbf{1} H^\top} \circ \nabla_W D_{KL}(V \| WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{\frac{V}{WH} H^\top}{\textbf{1} H^\top}
\end{equation}
Once again using the same reasoning as in the previous section, we can interpret the re-scaled gradient descent used in the MU updates as a gradient descent with a variable step size $\eta_k$ that is constrained to ensure the non-negativity of the iterates.
We derive the MU updates for $W$:
\begin{equation} \label{GD MU updates KL}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W D_{KL}(V \| W^{(k)}H)
\end{equation}
To ensure the non-negativity of the iterates, we impose the following condition:
\begin{equation}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W D_{KL}(V \| W^{(k)}H) \geq 0
\end{equation}
This leads to the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\nabla_W D_{KL}(V \| W^{(k)}H)} = \frac{W^{(k)}}{\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top}
\end{equation}
Since $\frac{V}{W^{(k)}H} H^\top \geq 0$ and $\textbf{1} H^\top \geq 0$, we can write the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\textbf{1} H^\top} < \frac{W^{(k)}}{\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top}
\end{equation}
The selecting the step size as:
\begin{equation}
    \eta_k = \frac{W^{(k)}}{\textbf{1} H^\top}
\end{equation}
and substituting in equation \ref{GD MU updates KL} we obtain the MU update for \( W \):
\begin{equation}
    W^{(k+1)} = W^{(k)} - \frac{W^{(k)}}{\textbf{1} H^\top} \circ (\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top)
\end{equation}
And simplifying we obtain the update rule for \( W \):
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{\frac{V}{W^{(k)}H} H^\top}{\textbf{1} H^\top}}
\end{equation}
Analogously, we can derive the MU update for \( H \) and using the gradient $\nabla_H D_{KL}(V \| WH) = W^\top \textbf{1} - W^\top \frac{V}{WH}$ we obtain:
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^{(k) \top} \frac{V}{W^{(k)} H^{(k)}}}{W^{(k) \top} \textbf{1}}}
\end{equation}
\par The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[H]\label{MU for KL}
    \caption{Multiplicative Updates for NMF with the Kullback-Leibler divergence}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} \frac{V}{W^{(k)} H^{(k)}}}{W^{(k) \top} 1} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{\frac{V}{W^{(k)} H^{(k+1)}} H^{(k+1) \top}}{1 H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the Kullback-Leibler divergence}
\begin{theorem}[\cite{lee2000algorithms}, Theorem 2] \label{MU convergence KL}
    The value of the objective function $D(V, WH) = D_{KL}(V \| WH)$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by Multiplicative updates \ref{MU for KL}.
    Furthermore $W^{(k)}$ and $H^{(k)}$ are stationary points of the objective $D(V, WH)$ if and only if the distance is invariant under the iterative scheme \ref{MU for KL}.
\end{theorem}
\begin{proof}
    TODO
\end{proof}
\subsubsection{Computational cost}
\section{Alternating Non-Negative Least Squares (ANLS)}
The alternating non-negative least squares (ANLS) method consists in solving exactly the non-negative least squares sub-problems at each iteration of the two-blocks coordinate descent scheme \ref{2-BCD}, this is a case of exact 2-BCD \ref{Exact 2-BCD}.
This scheme was originally proposed in \cite{Paatero1994} and further studied in \cite{kim2008nonnegative}.
\par
The metric used in this case is the Frobenius norm $D(V, WH) = \|V - WH\|_F^2$, and the subproblems to solve at each iteration are:
\begin{equation}
    W^{(k + 1)} = \arg\min_{W \geq 0} \|V - WH^{(k)}\|_F^2
\end{equation}
and
\begin{equation}
    H^{(k + 1)} = \arg\min_{H \geq 0} \|V - W^{(k)}H\|_F^2
\end{equation}
The algorithm can be summarized as follows:
\begin{algorithm}[H]\label{ANLS}
    \caption{Alternating Non-Negative Least Squares for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k + 1)} \leftarrow \arg\min_{W \geq 0} \|V - WH^{(k)}\|_F^2 \)
            \State \( H^{(k + 1)} \leftarrow \arg\min_{H \geq 0} \|V - W^{(k + 1)}H\|_F^2 \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\section{Projected methods}
    Since the non-negative least squares sub-problems are non trivial to solve, an idea is to solve the sub-problems without the non-negativity constraints and then project the solution onto the non-negative orthant.
    This leads to the projected gradient descent method for NMF where the solution of each subproblem takes the following form:
    \begin{equation}
        W^{(k + 1)} = \max\left(0, \arg\min_W \|V - W^{(k)}H^{(k)}\|_F^2  \right)
    \end{equation}
    and
    \begin{equation}
        H^{(k + 1)} = \max\left(0, \arg\min_H \|V - W^{(k + 1)}H\|_F^2  \right)
    \end{equation}
    \subsection{Alternating Least squares ALS}
The Alternating Least Squares (ALS) method is a projected method where the sub-problems are solved using the ordinary least squares solution \ref{least squares} and then projected onto the non-negative orthant.
This is very easy to implement since most programming languages have built-in functions to solve least squares problems. For instance in MATLAB one can use the backslash operator `$\backslash$' to solve least squares problems. 
In python, that is the language used in the repository associated with this thesis, the method \textit {`numpy.linalg.lstsq'    } is used to solve least squares problems.
\par
The ALS algorithm can be summarized as follows:
\begin{algorithm}[H]\label{ALS}
    \caption{Alternating Least Squares for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \
in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k + 1)} \leftarrow \max\left(0, \arg\min_W \|V - WH^{(k)}\|_F^2  \right) \)
            \State \( H^{(k + 1)} \leftarrow \max\left(0, \arg\min_H \|V -  W^{(k + 1)}H\|_F^2  \right) \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
This is the default solver used in MATLAB \footnote{Version R2025b.} for Non-negative Matrix Factorization via the function \textit{`nnmf'}. 
\subsubsection{Convergence}
Although the ALS algorithm is very popular in practice due to its simplicity, it does not have any convergence guarantees, in fact it has been observed that the algorithm can diverge in some cases. See \cite{gillis2020nonnegative} section 8.3.2.
\subsubsection{Computational cost}
The computational cost of the ALS algorithm depends on the cost of solving the least squares problems at each iteration.
Using the normal equations method \ref{normal equations} the cost of solving the least squares problem for \( W \) is $O(mnr + mr^2 + r^3)$ and for \( H \) is $O(mnr + nr^2 + r^3)$. 
Thus the total cost per iteration is $O(2mnr + (m+n)r^2 + 2r^3)$.
\subsection{Projected Gradient}
The projected gradient method consists in performing a gradient descent step for each block of variables and then projecting the result onto the non-negative orthant.
This method is discussed in a detailed way in \cite{lin2007projected}.
\par
As seen in section \ref{MU derivation section}, the gradient of the Frobenius norm with respect to \( H \) is given by:
\begin{equation}
    \nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top (WH - V) = W^\top WH - W^\top V
\end{equation}
and with respect to \( W \) is given by:
\begin{equation}
    \nabla_W \frac{1}{2} \|V - WH\|_F^2 = (WH - V) H^\top = W H H^\top - V H^\top
\end{equation}
In this case the step size is chosen to be the Lipschitz constant of the gradient.
The Lipschitz constant of the gradient with respect to \( H \)  is equal to \( \|W\|_2^2 \) and with respect to \( W \) is equal to \( \|H\|_2^2 \).
Thus the projected gradient updates for \( H \) and \( W \) are given by:
\begin{equation}
    H \leftarrow \max\left(0, H - \frac{1}{\|W\|_2^2} \nabla_H \frac{1}{2} \|V - WH\|_F^2 \right)
\end{equation}
and
\begin{equation}
    W \leftarrow \max\left(0, W - \frac{1}{\|H\|_2^2} \nabla_W \frac{1}{2   } \|V - WH\|_F^2 \right)
\end{equation}
The complete projected gradient algorithm is summarized in the following algorithm:
\begin{algorithm}[H]\label{Projected Gradient}
    \caption{Projected Gradient for NMF}
    \begin{algorithmic}[1] 
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State LW \( \leftarrow \|W^{(k)}\|_2^2 \)
            \State LH \( \leftarrow \|H^{(k)}\|_2^2 \)
            \Comment{Lipschitz constants}
            \State Grad\_W \( \leftarrow W^{(k)} H^{(k)} H^{(k) \top} - V H^{(k) \top} \)
            \Comment{Gradient w.r.t. W}
            \State $W^{(k+1)}$ \( \leftarrow \max\left(0, W^{(k)} - \frac{1}{LW} \text{Grad\_W} \right) \)
            \Comment{Projected gradient update for W}
            \State Grad\_H \( \leftarrow W^{(k+1) \top} W^{(k+1)} H^{(k)} - W^{(k+1) \top} V \)
            \Comment{Gradient w.r.t. H}
            \State $H^{(k+1)} \leftarrow \max\left(0, H^{(k)} - \frac{1}{LH} \text{Grad\_H} \right)$
            \Comment{Projected gradient update for H}
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence}
% this is a gd with lipschitz step size, project gradient descent convergence theorem in the background chapter
TODO
\subsubsection{Computational cost}
The computational cost discussion here is similar to the one presented for the MU algorithm with the Frobenius norm in section \ref{MU derivation section}, since the main cost is in the computation of the gradients.
\subsection{Hierarchical Alternating Least Squares (HALS)}
TODO
\section{MU with beta divergence} \label{MU beta section}
The $\beta$-divergence is a family of cost functions with a single parameter $\beta$ that generalizes several common cost functions used in NMF, including the Frobenius norm and the Kullback-Leibler divergence.
For the definition of the beta divergence see section \ref{Beta divergence section}.
\par
In this section we will derive the MU iteration scheme for $D(V, WH) = D_{\beta}(V, WH)$ the beta divergence in the general case.
One can see that the previous results for the Frobenius norm and the Kullback-Leibler divergence are special cases of the beta divergence for $\beta = 2$ and $\beta = 1$ respectively.
However, since they are the most popular cost functions used in NMF, we presented them separately.
Furthermore very few theoretical results are known for values of $\beta$ other than 0, 1 and 2.
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H D_{\beta}(V, WH) = W^\top \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right)
\end{equation}
Where the exponentiation is intended element-wise.
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top (WH)^{\beta - 1}} \circ \nabla_H D_{\beta}(V, WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top (V \circ (WH)^{\beta - 2})}{W^\top (WH)^{\beta - 1}}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W D_{\beta}(V, WH) = \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right) H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{(WH)^{\beta - 1} H^\top} \circ \nabla_W D_{\beta}(V, WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{(V \circ (WH)^{\beta - 2}) H^\top}{(WH)^{\beta - 1} H^\top}
\end{equation}
Ultimately, the updated are given by:
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{(V \circ (W^{(k)}H^{(k)})^{\beta - 2}) H^{(k) \top}}{(W^{(k)}H^{(k)})^{\beta - 1} H^{(k) \top}}}
\end{equation}
and
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}}}
\end{equation}
\par
The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[H]\label{MU for beta}
    \caption{Multiplicative Updates for NMF with the beta divergence}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \), parameter \( \beta \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{(V \circ (W^{(k)}H^{(k+1)})^{\beta - 2}) H^{(k+1) \top}}{(W^{(k)}H^{(k+1)})^{\beta - 1} H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the $\beta$-divergence}
For the cases where $\beta = 1$ theorem \ref{MU convergence KL} apply, and for $\beta = 2$ theorem \ref{MU convergence Fro} apply.
For others values of $\beta$ we need to adjust MU updated \ref{MU for beta} by a scaling factor $\gamma(\beta)$ \cite{févotte2011algorithmsnonnegativematrixfactorization} defined as:
\begin{equation}
    \gamma(\beta) =
    \begin{cases}
        1/(2 - \beta) & \text{if } \beta < 1 \\
        1 & \text{if } 1 \leq \beta \leq 2 \\
        1/(\beta - 1) & \text{if } \beta > 2
    \end{cases}
\end{equation}
And constraint the iterates to be greater than a small positive constant $\varepsilon > 0$ \cite{takahashi2014global}.
The reason for this is that [...]
\par
Now we are ready to state the more general convergence theorem for MU with the beta divergence:
\begin{theorem}[\cite{gillis2020nonnegative} theorem 8.9]
    Let $\varepsilon > 0$ and consider the adjusted beta divergence Multiplicative updates defined as:
    \begin{align}
        &H^{(k + 1)} \leftarrow \max\Bigg(\varepsilon, 
        H^{(k)} \circ \Big(\frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}}\Big)^{\gamma(\beta)}\Bigg)
        \\
        &W^{(k + 1)} \leftarrow \max\Bigg(\varepsilon, 
        W^{(k)} \circ \Big(\frac{(V \circ (W^{(k)}H^{(k+1)})^{\beta - 2}) H^{(k+1) \top}}{(W^{(k)}H^{(k+1)})^{\beta - 1} H^{(k+1) \top}}\Big)^{\gamma(\beta)}\Bigg)
    \end{align}
then: 
\begin{enumerate}
    \item The value of the objective function $D(V, WH) = D_{\beta}(V \| WH)$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by the adjusted beta divergence Multiplicative updates, given that $W \geq \varepsilon$ and $H \geq \varepsilon$.
    \item For any initialization $W^{(0)} \geq \varepsilon$ and $H^{(0)} \geq \varepsilon$, every limit point of the sequence $\{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}}$ generated by the adjusted beta divergence Multiplicative updates is a stationary point of the optimization problem $\min_{W,H \geq \varepsilon} D_{\beta}(V \| WH)$.
\end{enumerate}
\end{theorem}
\begin{proof}
    The proof of this theorem can be found in \cite{gillis2020nonnegative} section 8.2.4.
\end{proof}
\subsubsection{Computational cost}
