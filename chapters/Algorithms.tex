\chapter{Algorithms for NMF}\label{ALGO CHAPTER}
In this chapter all the algorithms studied in this thesis and used in the numerical simulations are derived and formally presented, along with their convergence properties when available.
The algorithms are listed here: Multiplicative Updated (MU) with the Frobenius norm \ref{MU for Frobenius}, Kullback-Leibler divergence \ref{MU for KL} and beta divergence \ref{MU for beta}; Alternating Least Squares (ALS) \ref{ALS}; Projected gradient descent (PGD) \ref{Projected Gradient}; Hierarchical Alternating Least Squares (HALS) \ref{HALS}.
\par
In section \ref{2-BCD general framework} the two-block coordinate descent framework is presented, which is the most common approach to design methods for solving the NMF optimization problem.
in section \ref{Stopping criteria section} the stopping criteria commonly used in the literature are discussed.
In section \ref{initialization section} the sensitivity of NMF algorithms to the initialization in general is highlighted, and the Non-negative Double Singular Value Decomposition (NNDSVD) initialization method is presented.
\par
Recall that the NMF problem \ref{NMF optimization problem} is NP-hard \cite{vavasis2007complexitynonnegativematrixfactorization}, hence all the Algorithms presented in this chapter are heuristics that aim to find a local minimum of the optimization problem.
More precisely, they are iterative schemes that start from an initial guess of the factors \( W_0 \) and \( H_0 \) and iteratively update them to reduce the value of the objective function, converging to a local minimum.
They don't guarantee to converge to the global minimum.
Furthermore the optimization problem \ref{NMF optimization problem} is a constrained optimization problem, since the factors \( W \) and \( H \) are constrained to be non-negative, that is \( W \geq 0 \) and \( H \geq 0 \).
To handle the non-negativity constraints, the algorithms presented in this chapter use two main approaches: the first one is to design a scheme that ensure the non-negativity of the iterates at each step, this is the case of the MU \ref{MU for Frobenius}, \ref{MU for KL} and \ref{MU for beta}, and Non-negative Least Squares (ANLS) \ref{ANLS};
the second approach is to first solve the sub-problems without the non-negativity constraints and then project the solution onto the non-negative orthant, this is the case of the ALS \ref{ALS}, HALS \ref{HALS}, and PGD \ref{Projected Gradient}.
\section{Two blocks coordinate descent (2-BCD)} \label{2-BCD general framework}
The most common setting for NMF is the so called two-blocks coordinate descent, in this setting the factors \( W \) and \( H \) are updated alternately, fixing one while updating the other.

\begin{algorithm}[ht]\label{2-BCD}
    \caption{Two-blocks coordinate descent for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \(W^{(k)}\) $\gets$ Update (\( W^{(k - 1)} \), \( H^{(k-1)} \))
            \Comment{Update \( W \) by fixing \( H \)}
            \State \(H^{(k)}\) $\gets$ Update (\( H^{(k - 1)} \), \( W^{(k)} \))
            \Comment{Update \( H \) by fixing \( W \)}
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
This general scheme is widely used, and we will present different ways to update the factors \( W \) and \( H \). There are mainly two observations that justify this approach:
\begin{enumerate}
    \item When one of the factors is fixed, the optimization subproblem is convex for most of the common cost functions. That makes it easier to design algorithms to solve them.
    \item Due to the symmetry of the problem $V = WH \iff V^T = H^T W^T$ the value of the cost function is the same if we swap \( W \) and \( H \) and transpose the input matrix \( V \). $D(V, WH) = D(V^T, H^T W^T)$.
    Therefore, it is sufficient to design an update rule for one of the factors, the other can be obtained by swapping the roles of \( W \) and \( H \) and transposing \( V \).
\end{enumerate}
Overall this approach is much easier to handle than trying to update both factors simultaneously. See \cite{mukkamala2019alternatingupdatesmatrixfactorization}.
\subsection{Exact 2-BCD}
If the subproblems of \ref{2-BCD} are solved exactly at each iteration, the method is called exact 2-BCD. 
And takes the following form:
\begin{algorithm}[ht]\label{Exact 2-BCD}
    \caption{Exact two-blocks coordinate descent for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k)} \leftarrow \arg\min_{W \geq 0} D(V, WH^{(k - 1)}) \)
            \State \( H^{(k)} \leftarrow \arg\min_{H \geq 0} D(V, W^{(k)}H) \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
In this case we have the following convergence result:
\begin{theorem}\label{Exact 2-BCD theorem}
    Let $\min_{W,H \geq 0} D(V, WH)$ be the NMF optimization problem, and
    \( \{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}} \) be the sequence generated by the exact 2-BCD method.
    if \begin{enumerate}
        \item The function \( D \) is continuously differentiable.
        \item Each block of variables $W$ and $H$ belong to a closed convex set. \label{Condition 2 Exact 2-BCD}
    \end{enumerate}
    Then the limit points of the sequence \( \{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}} \) are stationary points.
\end{theorem}
\begin{proof}
    this theorem is a corollary (Corollary 2) of the main theorem presented in \cite{GRIPPO2000127}.
\end{proof}
\begin{observation}
    In the NMF problem, the condition \ref{Condition 2 Exact 2-BCD} is always satisfied because the elements of $W$ and $H$ are constrained to lie in the non negative orthant.
\end{observation}
\section{Stopping criteria} \label{Stopping criteria section}
All the algorithms presented in this chapter are iterative methods that require a stopping criterion to terminate.
Usually, the implementation of these algorithms include a maximum number of iterations after which the algorithm stops.
However, others stopping criteria can be used in addition to the maximum number of iterations.
\par
A common stopping criterion is to check the relative error at each step and stop the algorithm if no significant improvement is observed for a given number of steps $T$.
For instance, we can use the following rule:
\begin{equation}
    |e(t - T) - e(t)| \leq tol\times e(t)
\end{equation}
Where $e(t)$ is the relative error at iteration $t$. This is equal to:
\begin{equation}  \label{stopping criterion I}
    e(t) = \frac{\|V - W^{(t)}H^{(t)}\|_F^2}{\|V\|_F^2}
\end{equation}
in the case of the Frobenius norm. 
$tol$ is a small tolerance value,  for instance $10^{-4}$, and $T$ is the lag used to compare the current error with the error $T$ iterates before.
\par
Another effective stopping criterion is to check the norm of the difference between the iterates and stop if it falls below a certain threshold.
\begin{align}
    &\|W^{(t)} - W^{(t-T)}\|_F^2 \leq tol \times \|W^{(t - T)}\|_F^2 \\
    &\|H^{(t)} - H^{(t-T)}\|_F^2 \leq tol \times \|H^{(t - T)}\|_F^2
\end{align}
Again a small value for $tol$, for instance $10^{-4}$, and $T = 10$ are reasonable choices in practice.
\par
The complete algorithm with the stopping criterion \ref{stopping criterion I} is summarized in the following pseudo-code:
\begin{algorithm}[ht]\label{2-BCD}
    \caption{Two-blocks coordinate descent for NMF with stopping criteria \ref{stopping criterion I}}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \), tolerance \( tol \), lag \( T \), maximum iterations \( \text{max\_iter} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \State Compute \( e(0) = \frac{\|V - W^{(0)}H^{(0)}\|_F^2}{\|V\|_F^2} \)
        \Comment{Initial relative error}
        \For {\( k = 0, 1, 2, \ldots, \text{max\_iter} \)}
            \State \(W^{(k)}\) $\gets$ Update (\( W^{(k - 1)} \), \( H^{(k-1)} \))
            \Comment{Update \( W \) by fixing \( H \)}
            \State \(H^{(k)}\) $\gets$ Update (\( H^{(k - 1)} \), \( W^{(k)} \))
            \Comment{Update \( H \) by fixing \( W \)}
            \State Compute \( e(k) = \frac{\|V - W^{(k)}H^{(k)}\|_F^2}{\|V\|_F^2} \)
            \Comment{Compute relative error}
            \If {\( k  \geq T \) \textbf{and} \( |e(k - T) - e(k)| \leq tol \times e(k) \)}
                \State \textbf{break}
                \Comment{If stopping criterion met, exit loop}
            \EndIf
        \EndFor
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\section{Initialization} \label{initialization section}
The algorithms commonly used for NMF are iterative methods that require an initial guess for the factors \( W^{(0)} \) and \( H^{(0)} \), they are generally sensitive to the initialization.
A very common yet effective initialization often used in the literature is to initialize \( W^{(0)} \) and \( H^{(0)} \) with random non-negative values sampled from a uniform distribution $[0,1]$.
\par
In this section we will present a slightly more sophisticated initialization method called Non-negative Double Singular Value Decomposition (NNDSVD) \cite{boutsidis2008svd}.
Recall the Eckart-Young theorem that states that the best rank-$r$ approximation of a matrix $V$ in the Frobenius norm sense is given by its truncated SVD \cite{schmidt1907theorie}; then the idea behind NNDSVD is to use the SVD of $V$ to compute a non-negative initialization for the factors \( W^{(0)} \) and \( H^{(0)} \).
\par
The algorithm can be summarized as follows:
\begin{algorithm}[ht]\label{NNDSVD}
    \caption{Non-negative Double Singular Value Decomposition (NNDSVD) for NMF initialization}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \)
        % \output \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \R_+^{r \times n} \) such that \( V \approx W^{(0)}H^{(0)} \)
        \State Compute the truncated SVD of \( V \): \( V_r = U_r \Sigma_r V_r^\top \)
        \State Initialize $W(:,1) = \sqrt{\sigma_1} U(:,1)$
        \State Initialize $H(1,:) = \sqrt{\sigma_1} V(:,1)^\top$
        \For{\( i = 2 \) to \( r \)}
            \State Let \( u = U(:,i) \), \( v = V(:,i) \)
            \State \( u^+ = \max(u, 0) \), \( u^- = \max(-u, 0) \)
            \State \( v^+ = \max(v, 0) \), \( v^- = \max(-v, 0) \)
            \State \( \|u^+\| = norm(u^+) \), \( \|u^-\| = norm(u^-) \)
            \State \( \|v^+\| = norm(v^+) \), \( \|v^-\| = norm(v^-) \)     
            \If {\( \|u^+\| \times \|v^+\| \geq \|u^-\| \times \|v^-\| \)}
                \State \( W(:,i) = \sqrt{\sigma_i} u^+ / \|u^+\| \)
                \State \( H(i,:) = \sqrt{\sigma_i} v^+ / \|v^+\|^\top \)
            \Else
                \State \( W(:,i) = \sqrt{\sigma_i} u^- / \|u^-\| \)
                \State \( H(i,:) = \sqrt{\sigma_i} v^- / \|v^-\|^\top \)
            \EndIf
        \EndFor    
        \State \Return \( W^{(0)}, H^{(0)} \)
    \end{algorithmic}
\end{algorithm}
See chapter \ref{Experiments chapter} section \ref{init comparison section} for a comparison between random and NNDSVD initializations with the HALS algorithm.
\section{Multiplicative Updates (MU)} \label{MU derivation section}
The multiplicative updates (MU) were first introduced by Lee and Seung in \cite{Lee1999}. They are one of the most popular algorithms for NMF.
In this section we will derive the MU algorithm for two different cost functions: the Frobenius norm and the Kullback-Leibler divergence.
The Lee and Seung paper with converge proof is \cite{lee2000algorithms}. A derivation using matrix algebra can be found in \cite{burred2014detailed}.
In section \ref{MU beta section} we will present the MU algorithm for the more general case of beta divergence \cite{févotte2011algorithmsnonnegativematrixfactorization}.
\subsection{MU with the Frobenius norm}
In this section we will derive the MU iteration scheme for the case where
$D(V, WH) = \|V - WH\|_F^2$ is the Frobenius norm.
The scheme can be interpreted as a scaled gradient descent method.\\
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top (WH - V) = W^\top WH - W^\top V
\end{equation}
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top WH} \circ \nabla_H \frac{1}{2} \|V - WH\|_F^2
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top V}{W^\top WH}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W \frac{1}{2} \|V - WH\|_F^2 = (WH - V) H^\top = W H H^\top - V H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{W H H^\top} \circ \nabla_W \frac{1}{2} \|V - WH\|_F^2
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{V H^\top}{W H H^\top}
\end{equation}
\par
Recall that in section \ref{Gradient descent section} we have introduce the gradient descent as an iterative method for finding local minima with the following update rule: $x_{k+1} = x_k - \eta_k\nabla f(x_k)$.
The re-scaled gradient descent used in the MU updates can be interpreted as a gradient descent with a variable step size $\eta_k$ that is constrained to ensure the non-negativity of the iterates.
We derive the MU updates for $W$:
\begin{equation} \label{GD MU updates}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2
\end{equation}
To ensure the non-negativity of the iterates, we impose the following condition:
\begin{equation}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2 \geq 0
\end{equation}
This leads to the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\nabla_W \frac{1}{2} \|V - W^{(k)}H\|_F^2} = \frac{W^{(k)}}{W^{(k)} H H^\top - V H^\top}
\end{equation}
Since $VH^\top \geq 0$ and $W^{(k)} H H^\top \geq 0$, we can write the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{W^{(k)} H H^\top} < \frac{W^{(k)}}{W^{(k)} H H^\top - V H^\top}
\end{equation}
The selecting the step size as:
\begin{equation}
    \eta_k = \frac{W^{(k)}}{W^{(k)} H H^\top}
\end{equation}
and substituting in equation \ref{GD MU updates} we obtain the MU update for \( W \):
\begin{equation}
    W^{(k+1)} = W^{(k)} - \frac{W^{(k)}}{W^{(k)} H H^\top} \circ (W^{(k)} H H^\top - V H^\top)
\end{equation}
And simplifying we obtain the update rule for \( W \):
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{V H^\top}{W^{(k)} H H^\top}}
\end{equation}
Analogously, we can derive the MU update for \( H \) and using the gradient $\nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top WH - W^\top V$ we obtain:
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^\top V}{W^\top W H^{(k)}}}
\end{equation}
\par The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[ht]\label{MU for Frobenius}
    \caption{Multiplicative Updates for NMF with the Frobenius norm}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} V}{W^{(k) \top} W^{(k)} H^{(k)}} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{V H^{(k+1) \top}}{W^{(k)} H^{(k+1)} H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the Frobenius norm}
\begin{theorem}[\cite{lee2000algorithms}, Theorem 1] \label{MU convergence Fro}
    The value of the objective function $D(V, WH) = \|V - WH \|_F^2$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by Multiplicative updates \ref{MU for Frobenius}.
    Furthermore $W^{(k)}$ and $H^{(k)}$ are stationary points of the objective $D(V, WH)$ if and only if the distance is invariant under the iterative scheme \ref{MU for Frobenius}.
\end{theorem}
\begin{proof}
    The proof of this theorem can be found in \cite{lee2000algorithms} section 6.
\end{proof}
\subsubsection{Computational cost}
The computational cost of the MU algorithm with the Frobenius norm depends on the computation of the gradients, $WHH^\top$ and $VH^\top$ for the update of $W$.
And $W^\top WH$ and $W^\top V$ for the update of $H$.
Recall that the dimensions of the matrices are:
\begin{itemize}
    \item \( V \in \mathbb{R}_+^{m \times n} \)
    \item \( W \in \mathbb{R}_+^{m \times r} \)
    \item \( H \in \mathbb{R}_+^{r \times n} \)
\end{itemize}
When computing $WHH^\top$ one should pay attention to compute $W(HH^\top)$ instead of $(WH)H^\top$ to reduce the computational cost.
The cost of the latter is $O(mnr + mnr^2)$ while the cost of the former is $O(mr^2 + r^2 n) = O(r^2(m+n))$. And in most NMF applications $r \ll \min(m,n)$.
The second term $VH^\top$ requires $O(mnr)$ operations.
\subsection{MU with the Kullback-Leibler divergence}
In this section we will derive the MU iteration scheme for the case where
$D(V, WH) = D_{KL}(V \| WH)$ is the Kullback-Leibler divergence. 
Again this scheme can be interpreted as a scaled gradient descent method, in fact more generally the MU updates can be interpreted as a scaled gradient descent method for a wide class of cost functions.
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H D_{KL}(V \| WH) = W^\top \left(\textbf{1} - \frac{V}{WH} \right) = W^\top \textbf{1} - W^\top \frac{V}{WH}
\end{equation}
Where $\textbf{1}$ is the matrix of all ones with the same dimensions as \( V \).
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top \textbf{1}} \circ \nabla_H D_{KL}(V \| WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top \frac{V}{WH}}{W^\top \textbf{1}}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W D_{KL}(V \| WH) = \left(\textbf{1} - \frac{V}{WH} \right) H^\top = \textbf{1} H^\top - \frac{V}{WH} H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{\textbf{1} H^\top} \circ \nabla_W D_{KL}(V \| WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{\frac{V}{WH} H^\top}{\textbf{1} H^\top}
\end{equation}
Once again using the same reasoning as in the previous section, we can interpret the re-scaled gradient descent used in the MU updates as a gradient descent with a variable step size $\eta_k$ that is constrained to ensure the non-negativity of the iterates.
We derive the MU updates for $W$:
\begin{equation} \label{GD MU updates KL}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W D_{KL}(V \| W^{(k)}H)
\end{equation}
To ensure the non-negativity of the iterates, we impose the following condition:
\begin{equation}
    W^{(k+1)} = W^{(k)} - \eta_k \nabla_W D_{KL}(V \| W^{(k)}H) \geq 0
\end{equation}
This leads to the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\nabla_W D_{KL}(V \| W^{(k)}H)} = \frac{W^{(k)}}{\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top}
\end{equation}
Since $\frac{V}{W^{(k)}H} H^\top \geq 0$ and $\textbf{1} H^\top \geq 0$, we can write the following inequality:
\begin{equation}
    \eta_k \leq \frac{W^{(k)}}{\textbf{1} H^\top} < \frac{W^{(k)}}{\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top}
\end{equation}
The selecting the step size as:
\begin{equation}
    \eta_k = \frac{W^{(k)}}{\textbf{1} H^\top}
\end{equation}
and substituting in equation \ref{GD MU updates KL} we obtain the MU update for \( W \):
\begin{equation}
    W^{(k+1)} = W^{(k)} - \frac{W^{(k)}}{\textbf{1} H^\top} \circ (\textbf{1} H^\top - \frac{V}{W^{(k)}H} H^\top)
\end{equation}
And simplifying we obtain the update rule for \( W \):
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{\frac{V}{W^{(k)}H} H^\top}{\textbf{1} H^\top}}
\end{equation}
Analogously, we can derive the MU update for \( H \) and using the gradient $\nabla_H D_{KL}(V \| WH) = W^\top \textbf{1} - W^\top \frac{V}{WH}$ we obtain:
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^{(k) \top} \frac{V}{W^{(k)} H^{(k)}}}{W^{(k) \top} \textbf{1}}}
\end{equation}
\par The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[ht]\label{MU for KL}
    \caption{Multiplicative Updates for NMF with the Kullback-Leibler divergence}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} \frac{V}{W^{(k)} H^{(k)}}}{W^{(k) \top} 1} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{\frac{V}{W^{(k)} H^{(k+1)}} H^{(k+1) \top}}{1 H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the Kullback-Leibler divergence}
\begin{theorem}[\cite{lee2000algorithms}, Theorem 2] \label{MU convergence KL}
    The value of the objective function $D(V, WH) = D_{KL}(V \| WH)$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by Multiplicative updates \ref{MU for KL}.
    Furthermore $W^{(k)}$ and $H^{(k)}$ are stationary points of the objective $D(V, WH)$ if and only if the distance is invariant under the iterative scheme \ref{MU for KL}.
\end{theorem}
\begin{proof}
    The proof of this theorem can be found in \cite{lee2000algorithms} section 6.
\end{proof}
\subsubsection{Computational cost}
In the case of the Kullback-Leibler divergence, the computational cost of the MU algorithm depends on the computation of the gradients, $\frac{V}{WH} H^\top$ and $\textbf{1} H^\top$ for the update of $W$.
The term $\textbf{1} H^\top$ can be computed in $O(mr)$ operations and the term $\frac{V}{WH} H^\top$ requires $O(mnr)$ operations.
\section{Alternating Non-Negative Least Squares (ANLS)}
The alternating non-negative least squares (ANLS) method consists in solving exactly the non-negative least squares sub-problems at each iteration of the two-blocks coordinate descent scheme \ref{2-BCD}, this is a case of exact 2-BCD \ref{Exact 2-BCD}.
This scheme was originally proposed in \cite{paatero1994positive}.
\par
The metric used in this case is the Frobenius norm $D(V, WH) = \|V - WH\|_F^2$, and the subproblem to solve at each iteration are:
\begin{equation}
    W^{(k + 1)} = \arg\min_{W \geq 0} \|V - WH^{(k)}\|_F^2
\end{equation}
and
\begin{equation}
    H^{(k + 1)} = \arg\min_{H \geq 0} \|V - W^{(k)}H\|_F^2
\end{equation}
The algorithm can be summarized as follows:
\begin{algorithm}[ht]\label{ANLS}
    \caption{Alternating Non-Negative Least Squares for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k + 1)} \leftarrow \arg\min_{W \geq 0} \|V - WH^{(k)}\|_F^2 \)
            \State \( H^{(k + 1)} \leftarrow \arg\min_{H \geq 0} \|V - W^{(k + 1)}H\|_F^2 \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
However solving exactly the constrained least-squares sub-problems is non-trivial; see \cite{kim2008nonnegative} for an example of an algorithm based on active-set methods to solve non-negative least squares problems.
\section{Projected methods}
    Since the non-negative least squares sub-problems \ref{ANLS} are non trivial to solve, a possible approach is to solve the sub-problems without the non-negativity constraints and then project the solution onto the non-negative orthant.
    This leads to the projected gradient descent methods for NMF where the solution of each subproblem takes the following form:
    \begin{equation}
        W^{(k + 1)} = \max\left(0, \arg\min_W \|V - W^{(k)}H^{(k)}\|_F^2  \right)
    \end{equation}
    and
    \begin{equation}
        H^{(k + 1)} = \max\left(0, \arg\min_H \|V - W^{(k + 1)}H^{(k)}\|_F^2  \right)
    \end{equation}
    \subsection{Alternating Least squares ALS}
The Alternating Least Squares (ALS) method is a projected method where the sub-problems are solved using the ordinary least squares solution \ref{least squares} and then projected onto the non-negative orthant.
This is very easy to implement since most programming languages have built-in functions to solve least squares problems. For instance in MATLAB one can use the backslash operator `$\backslash$' to solve least squares problems. 
In python, that is the language used in the repository associated with this thesis, the method \textit {`numpy.linalg.lstsq'    } is used to solve least squares problems.
\par
The ALS algorithm can be summarized as follows:
\begin{algorithm}[ht]\label{ALS}
    \caption{Alternating Least Squares for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \
in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( W^{(k + 1)} \leftarrow \max\left(0, \arg\min_W \|V - WH^{(k)}\|_F^2  \right) \)
            \State \( H^{(k + 1)} \leftarrow \max\left(0, \arg\min_H \|V -  W^{(k + 1)}H\|_F^2  \right) \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
This is the default solver used in MATLAB \footnote{Version R2025b.} for Non-negative Matrix Factorization via the function \textit{`nnmf'}. 
\subsubsection{Convergence}
Although the ALS algorithm is very popular in practice due to its simplicity, it does not have any convergence guarantees, in fact it has been observed that the algorithm can diverge in some cases. See \cite{gillis2020nonnegative} section 8.3.2.
\subsubsection{Computational cost}
The computational cost of the ALS algorithm depends on the cost of solving the least squares problems at each iteration.
Using the normal equations method \ref{normal equations} the cost of solving the least squares problem for \( W \) is $O(mnr + mr^2 + r^3)$ and for \( H \) is $O(mnr + nr^2 + r^3)$. 
Thus the total cost per iteration is $O(2mnr + (m+n)r^2 + 2r^3)$.
\subsection{Projected Gradient Descent (PGD)}
The projected gradient descent method consists in performing a gradient descent step for each block of variables and then projecting the result onto the non-negative orthant.
This method is discussed in a detailed way in \cite{lin2007projected}.
\par
As seen in section \ref{MU derivation section}, the gradient of the Frobenius norm with respect to \( H \) is given by:
\begin{equation}
    \nabla_H \frac{1}{2} \|V - WH\|_F^2 = W^\top (WH - V) = W^\top WH - W^\top V
\end{equation}
and with respect to \( W \) is given by:
\begin{equation}
    \nabla_W \frac{1}{2} \|V - WH\|_F^2 = (WH - V) H^\top = W H H^\top - V H^\top
\end{equation}
In this case the step size is chosen to be the Lipschitz constant of the gradient.
The Lipschitz constant of the gradient with respect to \( H \)  is equal to \( \|W\|_2^2 \) and with respect to \( W \) is equal to \( \|H\|_2^2 \).
Thus the projected gradient updates for \( H \) and \( W \) are given by:
\begin{equation}
    H \leftarrow \max\left(0, H - \frac{1}{\|W\|_2^2} \nabla_H \frac{1}{2} \|V - WH\|_F^2 \right)
\end{equation}
and
\begin{equation}
    W \leftarrow \max\left(0, W - \frac{1}{\|H\|_2^2} \nabla_W \frac{1}{2   } \|V - WH\|_F^2 \right)
\end{equation}
The complete projected gradient descent algorithm is summarized in the following algorithm:
\begin{algorithm}[ht]\label{Projected Gradient}
    \caption{Projected Gradient Descent for NMF}
    \begin{algorithmic}[1] 
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State LW \( \leftarrow \|W^{(k)}\|_2^2 \)
            \State LH \( \leftarrow \|H^{(k)}\|_2^2 \)
            \Comment{Lipschitz constants}
            \State Grad\_W \( \leftarrow W^{(k)} H^{(k)} H^{(k) \top} - V H^{(k) \top} \)
            \Comment{Gradient w.r.t. W}
            \State $W^{(k+1)}$ \( \leftarrow \max\left(0, W^{(k)} - \frac{1}{LW} \text{Grad\_W} \right) \)
            \Comment{Projected gradient update for W}
            \State Grad\_H \( \leftarrow W^{(k+1) \top} W^{(k+1)} H^{(k)} - W^{(k+1) \top} V \)
            \Comment{Gradient w.r.t. H}
            \State $H^{(k+1)} \leftarrow \max\left(0, H^{(k)} - \frac{1}{LH} \text{Grad\_H} \right)$
            \Comment{Projected gradient update for H}
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence}
% this is a gd with lipschitz step size, project gradient descent convergence theorem in the background chapter
This is a gradient descent method with a Lipschitz step size with a projection onto a the non-negative orthant when the iterates go out of bounds.
Thus theorem \ref{GD convergence theorem} can be applied to ensure the convergence of the projected gradient descent method for NMF.
\subsubsection{Computational cost}
The computational cost discussion here is similar to the one presented for the MU algorithm with the Frobenius norm in section \ref{MU derivation section}, since the main cost is in the computation of the gradients.
\subsection{Hierarchical Alternating Least Squares (HALS)}
The Hierarchical Alternating Least Squares (HALS) method is a projected method where each column of \( W \) and each row of \( H \) are updated sequentially by solving a non-negative least squares problem for each column/row while keeping the other columns/rows fixed.
The algorithm was originally proposed in \cite{cichocki2007hierarchical}.
\par
The main idea derive from the observation that for a fixed column $V[:,j] = v$ of the matrix \( V \), the optimization problem
\begin{equation}
    \min_{h} \| v - wh \|_2^2
\end{equation}
Has a closed form solution when $w$ is fixed.
That is, we can analytically solve for $h$ obtaining:
\begin{align}
    \| v - wh \|_2^2 &= (v - wh)^\top (v - wh) \\
    &= v^\top v - 2h w^\top v + (wh)^\top (wh) 
\end{align}
Taking the derivative with respect to $h$ and setting it to zero we obtain:
\begin{align}
    \frac{\partial}{\partial h} v^\top v - 2h w^\top v + (wh)^\top (wh) &= -2v^\top w + 2h w^\top w = 0 \\
    \Rightarrow h &= \frac{v^\top w}{w^\top w}
\end{align}
Note that in the unconstrained case the optimal solution can also be negative, however we can apply the projection idea to obtain a non-negative solution.
Thus the non-negative solution is given by:
\begin{equation}
    h = \max\left(0, \frac{v^\top w}{w^\top w} \right)
\end{equation}
Similarly, for a fixed row $V[i,:] = v^\top$ of the matrix \( V \), the optimization problem
\begin{equation}
    \min_{w} \| v^\top - w h^\top \|_2^2
\end{equation}
Has a closed form solution when $h$ is fixed.
\begin{align}
    \| v^\top - w h^\top \|_2^2 &= (v^\top - w h^\top)^\top (v^\top - w h^\top) \\
    &= v v^\top - 2w h v^\top + (w h^\top)^\top (w h^\top) 
\end{align}
Taking the derivative with respect to $w$ and setting it to zero we obtain:
\begin{align}
    \frac{\partial}{\partial w} v v^\top - 2w h v^\top + (w h^\top)^\top (w h^\top) &= -2 v h^\top + 2 w h h^\top = 0 \\
    \Rightarrow w &= \frac{v h^\top}{h h^\top}
\end{align}
That projected onto the non-negative orthant gives:
\begin{equation}
    w = \max\left(0, \frac{v h^\top}{h h^\top} \right)
\end{equation}
\par
Now we apply this idea to the NMF problem. Let $H[\ell,:]$ be the $l$-th row of the matrix \( H \) and $W[:,\ell]$ be the $l$-th column of the matrix \( W \);
the optimization problem for a fixed row $H[\ell,:]$ became:
\begin{equation} \label{HALS optimization problem}
    \min_{H[\ell,:] \geq 0} \| V - \sum_{k\not=\ell}W[:,k]H[k,:] - W[:,\ell]H[\ell,:] \|_F^2
\end{equation}
For simplicity we call the quantity $\tilde{R}_{\ell} = V - \sum_{k\not=\ell}W[:,k]H[k,:]$ the residual matrix with respect to the rank one matrix $W[:,\ell]H[\ell,:]$.
Then we can rewrite the problem \ref{HALS optimization problem} as:
\begin{align}
    \min_{H[\ell,:] \geq 0} \| \tilde{R}_{\ell} - W[:,\ell]H[\ell,:] \|_F^2 = \\
    \min_{H[\ell,:] \geq 0} \sum_{j=1}^n \| \tilde{R}_{\ell}[:,j] - W[:,\ell]H[\ell,j] \|_2^2
\end{align}
This has the same form as the problem solved previously for a fixed column of \( V \), thus the solution is given by:
\begin{equation}
    H[\ell,j] = \max\left(0, \frac{\tilde{R}_{\ell}[:,j]^\top W[:,\ell]}{W[:,\ell]^\top W[:,\ell]} \right)
\end{equation}
Similarly, for updating the column $W[:,\ell]$ we obtain:
\begin{equation}
    W[:,\ell] = \max\left(0, \frac{\tilde{R}_{\ell} H[\ell,:]^\top}{H[\ell,:] H[\ell,:]^\top} \right)
\end{equation}
Where now $\tilde{R}_{\ell} = V - \sum_{k\not=\ell}W[:,k]H[k,:]$ is the residual matrix with respect to the rank one matrix $W[:,\ell]H[\ell,:]$.
\par
The complete HALS algorithm is summarized in the following algorithm:
\begin{algorithm}[ht]\label{HALS}
    \caption{Hierarchical Alternating Least Squares for NMF}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \For{\( \ell = 1 \) to \( r \)}
                \State \( \tilde{R}_{\ell} \leftarrow V - \sum_{k\not=\ell}W[:,k]H[k,:] \)
                \State \( H[\ell,:] \leftarrow \max\left(0, \frac{\tilde{R}_{\ell}^\top W[:,\ell]}{W[:,\ell]^\top W[:,\ell]} \right) \)
            \EndFor
            \For{\( \ell = 1 \) to \( r \)}
                \State \( \tilde{R}_{\ell} \leftarrow V - \sum_{k\not=\ell}W[:,k]H[k,:] \)
                \State \( W[:,\ell] \leftarrow \max\left(0, \frac{\tilde{R}_{\ell} H[\ell,:]^\top}{H[\ell,:] H[\ell,:]^\top} \right) \)
            \EndFor
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence}
The convergence of the HALS algorithm to a stationary point of the NMF problem with the Frobenius norm is guaranteed by Theorem \ref{Exact 2-BCD theorem} since HALS is an instance of exact 2-BCD.
\subsubsection{Computational cost}
The comoputational cost of the HALS algorithm is esentially the same of the MU algorithm with the Frobenius norm discussed in section \ref{MU derivation section}, since the main cost is in the computation of the residual matrices $\tilde{R}_{\ell}$
the cost is $O(mnr)$ per iteration. In practice HALS is faster than MU while having the same computational cost per iteration. See \cite{gillis2020nonnegative} 8.3.3.
\section{MU with beta divergence} \label{MU beta section}
The $\beta$-divergence is a family of cost functions with a single parameter $\beta$ that generalizes several common cost functions used in NMF, including the Frobenius norm and the Kullback-Leibler divergence.
For the definition of the beta divergence see section \ref{Beta divergence section}.
\par
In this section we will derive the MU iteration scheme for $D(V, WH) = D_{\beta}(V, WH)$ the beta divergence in the general case.
One can see that the previous results for the Frobenius norm and the Kullback-Leibler divergence are special cases of the beta divergence for $\beta = 2$ and $\beta = 1$ respectively.
However, since they are the most popular cost functions used in NMF, we presented them separately.
Furthermore very few theoretical results are known for values of $\beta$ other than 0, 1 and 2.
\par
The gradient of the cost function with respect to \( H \) is given by:
\begin{equation}
    \nabla_H D_{\beta}(V, WH) = W^\top \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right)
\end{equation}
Where the exponentiation is intended element-wise.
A re-scaled gradient descent update for \( H \) is given by:
\begin{equation}
    H \leftarrow H - \frac{H}{W^\top (WH)^{\beta - 1}} \circ \nabla_H D_{\beta}(V, WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    H \leftarrow H \circ \frac{W^\top (V \circ (WH)^{\beta - 2})}{W^\top (WH)^{\beta - 1}}
\end{equation}
Similarly, the update rule for \( W \) can be derived by fixing \( H \)
\begin{equation}
    \nabla_W D_{\beta}(V, WH) = \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right) H^\top
\end{equation}
A re-scaled gradient descent update for \( W \) is given by:
\begin{equation}
    W \leftarrow W - \frac{W}{(WH)^{\beta - 1} H^\top} \circ \nabla_W D_{\beta}(V, WH)
\end{equation}
This leads to the following update rule:
\begin{equation}
    W \leftarrow W \circ \frac{(V \circ (WH)^{\beta - 2}) H^\top}{(WH)^{\beta - 1} H^\top}
\end{equation}
Ultimately, the updated are given by:
\begin{equation}
    \boxed{W^{(k+1)} = W^{(k)} \circ \frac{(V \circ (W^{(k)}H^{(k)})^{\beta - 2}) H^{(k) \top}}{(W^{(k)}H^{(k)})^{\beta - 1} H^{(k) \top}}}
\end{equation}
and
\begin{equation}
    \boxed{H^{(k+1)} = H^{(k)} \circ \frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}}}
\end{equation}
\par
The complete MU algorithm is summarized in the following algorithm:
\begin{algorithm}[ht]\label{MU for beta}
    \caption{Multiplicative Updates for NMF with the beta divergence}
    \begin{algorithmic}[1]
        \Require \( V \in \mathbb{R}_+^{m \times n} \), rank \( r \), initial factors \( W^{(0)} \in \mathbb{R}_+^{m \times r} \), \( H^{(0)} \in \mathbb{R}_+^{r \times n} \), parameter \( \beta \)
        % \output \( W \in \mathbb{R}_+^{m \times r} \), \( H \in \R_+^{r \times n} \) such that \( V \approx WH \)
        \While{stopping criterion not met}
            \State \( H^{(k+1)} \leftarrow H^{(k)} \circ \frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}} \)
            \State \( W^{(k+1)} \leftarrow W^{(k)} \circ \frac{(V \circ (W^{(k)}H^{(k+1)})^{\beta - 2}) H^{(k+1) \top}}{(W^{(k)}H^{(k+1)})^{\beta - 1} H^{(k+1) \top}} \)
        \EndWhile
        \State \Return \( W, H \)
    \end{algorithmic}
\end{algorithm}
\subsubsection{Convergence of MU with the $\beta$-divergence}
For the cases where $\beta = 1$ theorem \ref{MU convergence KL} apply, and for $\beta = 2$ theorem \ref{MU convergence Fro} apply.
For others values of $\beta$ we need to adjust MU updated \ref{MU for beta} by a scaling factor $\gamma(\beta)$ \cite{févotte2011algorithmsnonnegativematrixfactorization} defined as:
\begin{equation}
    \gamma(\beta) =
    \begin{cases}
        1/(2 - \beta) & \text{if } \beta < 1 \\
        1 & \text{if } 1 \leq \beta \leq 2 \\
        1/(\beta - 1) & \text{if } \beta > 2
    \end{cases}
\end{equation}
And constraint the iterates to be greater than a small positive constant $\varepsilon > 0$ \cite{takahashi2014global}.
The reason for this is that in this case the objective function $D_{\beta}(V \| WH)$ is guaranteed to be non-increasing under the adjusted MU update rules.
This result is obtained by \cite{févotte2011algorithmsnonnegativematrixfactorization} using the majorization-minimization framework.
\par
Now we are ready to state the more general convergence theorem for MU with the beta divergence:
\begin{theorem}[\cite{gillis2020nonnegative} theorem 8.9]
    Let $\varepsilon > 0$ and consider the adjusted beta divergence Multiplicative updates defined as:
    \begin{align}
        &H^{(k + 1)} \leftarrow \max\Bigg(\varepsilon, 
        H^{(k)} \circ \Big(\frac{W^{(k) \top} (V \circ (W^{(k)} H^{(k)})^{\beta - 2})}{W^{(k) \top} (W^{(k)} H^{(k)})^{\beta - 1}}\Big)^{\gamma(\beta)}\Bigg)
        \\
        &W^{(k + 1)} \leftarrow \max\Bigg(\varepsilon, 
        W^{(k)} \circ \Big(\frac{(V \circ (W^{(k)}H^{(k+1)})^{\beta - 2}) H^{(k+1) \top}}{(W^{(k)}H^{(k+1)})^{\beta - 1} H^{(k+1) \top}}\Big)^{\gamma(\beta)}\Bigg)
    \end{align}
then: 
\begin{enumerate}
    \item The value of the objective function $D(V, WH) = D_{\beta}(V \| WH)$ is non-increasing when $W^{(k)}, H^{(k)}$ are produced by the adjusted beta divergence Multiplicative updates, given that $W \geq \varepsilon$ and $H \geq \varepsilon$.
    \item For any initialization $W^{(0)} \geq \varepsilon$ and $H^{(0)} \geq \varepsilon$, every limit point of the sequence $\{(W^{(k)}, H^{(k)})\}_{k \in \mathbb{N}}$ generated by the adjusted beta divergence Multiplicative updates is a stationary point of the optimization problem $\min_{W,H \geq \varepsilon} D_{\beta}(V \| WH)$.
\end{enumerate}
\end{theorem}
\begin{proof}
    The proof of this theorem can be found in \cite{gillis2020nonnegative} section 8.2.4.
\end{proof}
\subsubsection{Computational cost}
The computational cost is given by the cost of computing the two gradients at each iteration, recall that the gradient with respect to \( H \) is given by:
\begin{equation}
    \nabla_H D_{\beta}(V, WH) = W^\top \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right)
\end{equation}
and with respect to \( W \) is given by:
\begin{equation}
    \nabla_W D_{\beta}(V, WH) = \left( (WH)^{\beta - 1} - V \circ (WH)^{\beta - 2} \right) H^\top
\end{equation}
The cost of computing the gradient with respect to \( H \) is $O(mnr)$ and the cost of computing the gradient with respect to \( W \) is also $O(mnr)$.
\section{Summary and extensions}
During the last decades, many algorithm have been proposed to solve the NMF problem, 
in this chapter we derived and presented the general framework for designing NMF algorithms \ref{2-BCD},
and derived the most important ones, namely the multiplicative updates with beta divergence \ref{MU for beta}, which includes the MU for the Frobenius norm and the Kullback-Leibler divergence as special cases for $\beta = 2$ and $\beta = 1$ respectively, the alternating non-negative least squares method \ref{ANLS}, the alternating least squares method \ref{ALS}, the projected gradient descent method \ref{Projected Gradient}, and the hierarchical alternating least squares method \ref{HALS}.
There are different variant of the algorithm presented here, notably some algorithm can be accelerated, \cite{gillis2012accelerated}.
Furthermore, all the algorithms presented here are part of a class of algorithms called first-order methods, that is, they only use the first order information of the objective function, however there are also second-order methods that use the second order information of the objective function,in the context of NMF, first-order methods are the most popular ones
however some second-order methods have been proposed in the literature, see \cite{huang2019low} and \cite{fu2020computing} for a more general survey.
Finally, one can observe that all the algorithms presented here compute the gradient of the objective function exactly at each iteration, 
however one can wonder what happens if we decide to compute the gradient approximately, by just using a subset of the data, this leads to the class of stochastic algorithms for NMF, that are particularly useful when the data matrix \( V \) is very large.
One obvious fact is that the computational cost at each iteration will be reduced, in particular suppose that we want to compute the gradient of the objective function with respect to $H$, then we consider the objective function with a subset $I$ of the columns of \( V \) and \( W \):
$\|V[:,I] - W H[:,I]\|_F^2$ then the cost of computing the gradient will be $O(|I|nr)$ instead of $O(mnr)$, usually with $|I| \ll m$.