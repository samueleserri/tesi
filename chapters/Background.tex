\chapter{Background}
The aim of this chapter is to introduce the mathematical background and notation necessary for understanding the Non-negative Matrix Factorization (NMF) problem, and for deriving the algorithms to tackle it in a 
way such that the content is self-contained, and all the concepts used in the subsequent chapters are properly defined.
\par
In particular this chapter is organized as follows: first in section \ref{Linear algebra and matrix decomposition section} the basic concepts of linear algebra are introduced, specifically the problem of matrix decomposition is presented together with the most known decomposition technique: the Singular Value Decomposition (SVD). This last result will be
used when discussing some technique for the initialization of the factors in NMF algorithms in chapter \ref{ALGO CHAPTER}.
Then in section \ref{optimization section} the taxonomy of optimization problems is presented along with some important results regarding the Least Squares problem \ref{least squares} and the Gradient Descent algorithm \ref{Gradient descent section}.
\par
Clearly this chapter is not mean to be an exhaustive introduction to linear algebra and optimization, and it is assumed that the reader is already familiar with most of the concepts presented here, therefore only those results that are essential are included. 
Finally the point of view adopted here is mainly computational and from a data analysis perspective.
\par
The main reference for organizing this chapter has been the book by Stephen Wright and Benjamin Recht \cite{wright2022optimization}.
\section{Linear Algebra and Matrix decomposition}\label{Linear algebra and matrix decomposition section}
This section briefly introduces the common [find the correct word] of linear algebra from scratch;
in section \ref{matrix decomposition section} the problem of matrix decomposition is introduced from a general perspective and the Singular Value Decomposition (SVD) is presented, it is shown how the SVD can be equivalently expressed as a sum of rank-one matrices and the Eckart-Young theorem \ref{Eckart-Young Theorem} is stated.
\subsection{Basics of Linear Algebra}
\begin{definition}[Matrix]
    Given two numbers $m,n \in N$, a matrix $A \in \R^{m \times n}$ is a rectangular array of real numbers with $m$ rows and $n$ columns.
\end{definition}
\begin{equation*}
    A = \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
\end{equation*}
\begin{definition}[transpose of a matrix]
    Given a matrix $A \in \R^{m \times n}$, the transpose of $A$, denoted by $A^\top$, is the matrix obtained by swapping the rows and columns of $A$.
    That is, the matrix $A^\top$ is a matrix such that $A[i,j] = A^\top[j,i]$ for all $1 \leq i \leq m$ and $1 \leq j \leq n$.
\end{definition}
\begin{observation}
    The transpose of a matrix $A \in \R^{m \times n}$ is a matrix $A^\top \in \R^{n \times m}$.
\end{observation}
\begin{definition}[Product of matrices]
    Given two matrices $A \in \R^{m \times n}$ and $B \in \R^{n \times p}$, the product of $A$ and $B$, denoted by $AB$, is the matrix $C \in \R^{m \times p}$ such that:
    \begin{equation*}
    C[i,j] = \sum_{k=1}^{n} A[i,k] B[k,j]
    \end{equation*}
    for all $1 \leq i \leq m$ and $1 \leq j \leq p$.
\end{definition}
\begin{definition}[Column vector]
    A column vector $x \in \R^m$ is a matrix with $m$ rows and 1 column. (i.e. $x \in \R^{m \times 1}$)
\end{definition}
\begin{terminology}
    In this text the word vector will always refer to a column vector unless otherwise specified.
\end{terminology}
\begin{equation*}
    x = \begin{bmatrix}
    x_{1} \\
    x_{2} \\
    \vdots \\
    x_{m}
    \end{bmatrix}
\end{equation*}
\begin{definition}[Row vector]
    A row vector $y \in \R^n$ is the transpose of a column vector. (i.e. $y \in \R^{1 \times n}$)
\end{definition}
\begin{equation*}
    y =\begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
    \end{bmatrix}^\top =\begin{bmatrix}
    y_{1} & y_{2} & \cdots & y_{n}
    \end{bmatrix}
\end{equation*}
\subsubsection{Norms}
\begin{definition}[Norm of a vector]
    Given a vector $x \in \R^n$, a norm is a function $\| \cdot \|: \R^n \rightarrow \R_+$ that satisfies the following properties:
    \begin{itemize}
        \item $\| x \| \geq 0$ for all $x \in \R^n$ and $\| x \| = 0$ if and only if $x = 0$.
        \item $\| \alpha x \| = |\alpha| \| x \|$ for all $\alpha \in \R$ and $x \in \R^n$.
        \item $\| x + y \| \leq \| x \| + \| y \|$ for all $x, y \in \R^n$ (triangle inequality).
    \end{itemize}
\end{definition}
\begin{example}
    The Euclidean norm (or 2-norm) of a vector $x \in \R^n$ is defined as:
    
    \begin{equation}
    \| x \|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
    \end{equation}
    The 1-norm of a vector $x \in \R^n$ is defined as:
    \begin{equation}
    \| x \|_1 = \sum_{i=1}^{n} |x_i|
    \end{equation}
    The infinity norm of a vector $x \in \R^n$ is defined as:
    \begin{equation}
    \| x \|_\infty = \max_{1 \leq i \leq n} |x_i|
    \end{equation}
\end{example}
\begin{definition}[Norm of a matrix]
    Given a matrix $A \in \R^{m \times n}$, a norm is a function $\| \cdot \|: \R^{m \times n} \rightarrow \R_+$ that satisfies the following properties:
    \begin{itemize}
        \item $\| A \| \geq 0$ for all $A \in \R^{m \times n}$ and $\| A \| = 0$ if and only if $A = 0$.
        \item $\| \alpha A \| = |\alpha| \| A \|$ for all $\alpha \in \R$ and $A \in \R^{m \times n}$.
        \item $\| A + B \| \leq \| A \| + \| B \|$ for all $A, B \in \R^{m \times n}$ (triangle inequality).
    \end{itemize}
\end{definition}
\begin{example}
    The Frobenius norm of a matrix $A \in \R^{m \times n}$ is defined as:
    \begin{equation}
    \| A \|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}^2}
    \end{equation}
    The 1-norm of a matrix $A \in \R^{m \times n}$ is defined as:
    \begin{equation}
    \| A \|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}|
    \end{equation}
    The infinity norm of a matrix $A \in \R^{m \times n}$ is defined as:
    \begin{equation}
    \| A \|_\infty = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}|
    \end{equation}
\end{example}
\subsection{Matrix decomposition} \label{matrix decomposition section}
TODO.
\subsubsection{Singular Value Decomposition (SVD)}
The Singular Value decomposition (SVD) of a matrix $A \in \R^{m \times n}$ takes the form:
\begin{equation}
    A = U \Sigma V^\top = 
\begin{bmatrix}u_{11}&\cdots&u_{1m}\\\vdots&\ddots&\vdots\\u_{m1}&\cdots&u_{mm}\end{bmatrix}
\begin{bmatrix}
\sigma_1 & & & 0\\
& \ddots & & \vdots\\
& & \sigma_r & \\
0 & \cdots & & 0
\end{bmatrix}_{m\times n}
\begin{bmatrix}v_{11}&\cdots&v_{1n}\\\vdots&\ddots&\vdots\\v_{n1}&\cdots&v_{nn}\end{bmatrix}
\end{equation}
where \begin{itemize}
    \item $U \in \R^{m \times m}$ is the orthogonal matrix that diagonalizes $A^\top A$. The columns of $U$ are called the left singular vectors of $A$.
    \item $\Sigma \in \R^{m \times n}$ is a diagonal matrix whose diagonal entries are the singular values of $A$, arranged in descending order.
    \item $V \in \R^{n \times n}$ is the orthogonal matrix that diagonalizes $AA^\top$. The columns of $V$ are called the right singular vectors of $A$.
\end{itemize}
\begin{observation}
    We can equivalently write the SVD as a sum of rank-one matrices:
    \begin{equation}
        A = \sum_{i=1}^{r} \sigma_i u_i v_i^\top
    \end{equation}
    where $r$ is the rank of $A$, $\sigma_i$ are the singular values, and $u_i$ and $v_i$ are the corresponding left and right singular vectors.
\end{observation}
We have the following important theorem regarding the SVD:
\begin{theorem}[Eckart-Young Theorem] \label{Eckart-Young Theorem}
    Let $A \in \R^{m \times n}$ be a matrix with singular value decomposition $A = U \Sigma V^\top$. For any integer $k$ such that $1 \leq k \leq rank(A)$, the best rank-$k$ approximation of $A$ in terms of the Frobenius norm is given by:
    \begin{equation}
        A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^\top
    \end{equation}
\end{theorem}

\section{Optimization} \label{optimization section}
In this section the taxonomy of optimization problems is introduced: we give the definition of an optimization problem in its general form \ref{optimization problem definition}, and the definitions of global \ref{global optimum definition} and local \ref{local optimum definition} optima; then the 
Least Squares problem is presented \ref{least squares} along with its closed-form solution \ref{normal equations}, this result will be useful in designing some of the algorithms presented in chapter \ref{ALGO CHAPTER}.
Finally, the gradient descent algorithm \ref{Gradient descent section} is introduced along with a convergence result for general smooth functions \ref{GD convergence theorem}.
\subsection*{}
\begin{definition}[Optimization problem] \label{optimization problem definition}
    An optimization problem is defined as:
    \begin{equation}
        \min_{x \in X}f(x)
    \end{equation}
    where \begin{itemize}
        \item $X$ is the feasible set in which the solution must lie.
        \item $f: X \rightarrow \R$ is the objective function to be minimized
    \end{itemize}
\end{definition}
\begin{terminology}
    In other context the objective function is also referred to as cost function, loss function, or energy function. Here will use them interchangeably.
\end{terminology}
\begin{definition}[Global optimum]\label{global optimum definition}
    A point $x^* \in X$ is a global optimum of the optimization problem $\min_{x \in X}f(x)$ if:
    \begin{equation}
        f(x^*) \leq f(x) \quad \forall x \in X
    \end{equation}
\end{definition}
\begin{definition}[Local optimum]\label{local optimum definition}
    A point $x^* \in X$ is a local optimum of the optimization problem $\min_{x \in X}f(x)$ if there exists a neighborhood $U$ of $x^*$ such that:
    \begin{equation}
        f(x^*) \leq f(x) \quad \forall x \in U \cap X
    \end{equation}
\end{definition}
\begin{example}[Linear least squares problem]\label{least squares}
    Given a matrix $A \in \R^{m \times n}$ and a vector $b \in \R^m$, the linear least squares problem is defined as:
    \begin{equation}
        \min_{x \in \R^n} \| Ax - b \|_2^2
    \end{equation}  
\end{example}
\begin{theorem}[solution of the linear least squares problem] \label{normal equations}
    The solution of the linear least squares problem $\min_{x \in \R^n} \| Ax - b \|_2^2$ is given by:
    \begin{equation}
        x^* = (A^\top A)^{-1} A^\top b = A^\dagger A^\top b
    \end{equation}
    provided that $A^\top A$ is invertible.
    This solution is also known as the normal equations solution.
\end{theorem}
\begin{proof}
    The objective function can be rewritten as:
    \begin{equation*}
        f(x) = \| Ax - b \|_2^2 = (Ax - b)^\top (Ax - b) = x^\top A^\top A x - 2b^\top A x + b^\top b
    \end{equation*}
    To find the minimum, we compute the gradient of $f(x)$ and set it to zero:
    \begin{equation*}
        \nabla f(x) = 2A^\top A x - 2A^\top b
    \end{equation*}
    Setting the gradient to zero gives:
    \begin{equation*}
        2A^\top A x - 2A^\top b = 0
    \end{equation*}
    which simplifies to:
    \begin{equation*}
        A^\top A x = A^\top b
    \end{equation*}
    Assuming that $A^\top A$ is invertible, we can solve for $x$:
    \begin{equation*}
        x^* = (A^\top A)^{-1} A^\top b
    \end{equation*}
    This completes the proof.
\end{proof}
% illustration of least squares
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Imgs/Least_squares.png}
    \caption{In a three-dimensional setting
    the least squares regression line becomes a plane. The plane is chosen
to minimize the sum of the squared vertical distances between each vector
(shown in red) and the plane.
\\  \vspace{2pt} \small\textit{Image from: \cite{IntroToSL}}}
    \label{fig:least_squares}
\end{figure}
\newpage
\subsection{Gradient descent}\label{Gradient descent section}
Often optimization problems do not admit an analytical solution, therefore iterative procedures are used to find a local minimum of the objective function.
Gradient descent is one of the so called first-order iterative algorithms, that is an algorithm that only uses the first derivative of the objective function to compute the next iterate.
The high-level idea of this method is to iteratively update the position of the current point by taking a step in the opposite direction of the gradient of the objective function at that point, since the gradient points in the direction of the steepest ascent.
\begin{definition}[Gradient descent]
    Gradient descent is an iterative optimization algorithm used to find a local minimum of a differentiable function. Given an initial point $x_0 \in X$, the algorithm updates the point iteratively using the following rule:
    \begin{equation}
        x_{k+1} = x_k - \eta_k \nabla f(x_k)
    \end{equation}
    where $\eta_k > 0$ is the step size (or learning rate) at iteration $k$ and $\nabla f(x_k)$ is the gradient of the objective function at point $x_k$.
\end{definition}
We have the following convergence result for gradient descent:
\begin{theorem} [GD convergence for a general smooth function] \label{GD convergence theorem} 
    Let $f: \R^n \rightarrow \R$ be a differentiable function with $L$-Lipschitz continuous gradient, i.e., there exists a constant $L > 0$ such that for all $x, y \in \R^n$:
    \begin{equation*}
        \| \nabla f(x) - \nabla f(y) \|_2 \leq L \| x - y \|_2
    \end{equation*}
    Let $\{x_k\}$ be the sequence generated s.t. $f(x_{k+1}) \leq f(x_k) - \frac{1}{2L} \| \nabla f(x_k) \|_2^2$; and suppose that $f$ is bounded below, then we have:
    \begin{equation*}
        f(x_k) \rightarrow f(\bar{x}) \quad \text{as} \quad k \rightarrow \infty
    \end{equation*}
    where $\bar{x}$ is a stationary point of $f$, i.e., $\nabla f(\bar{x}) = 0$.
\end{theorem}
