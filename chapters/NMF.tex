\chapter{Non-Negative Matrix Factorization}
This chapter introduces the NMF problem.
\section{Definition}
In its more general form the Non-negative matrix factorization problem can be defined as follows: given a non-negative matrix $V \in \R_+^{m \times n}$ and a factorization rank $r \in \N$ find two matrices
$W \in \R_+^{m \times r}$ and $H \in \R_+^{r \times n}$, constrained to be non-negative, such that their product $WH$ approximates $V$ as closely as possible according to some distance measure $D$.
\par
This problem can be formulated with an optimization problem as follows:
\begin{definition}[Non-Negative Matrix Factorization optimization problem]\label{NMF optimization problem}
Given a non-negative matrix $V \in \R_+^{m \times n}$ and a factorization rank $r \in \N$, find two non-negative matrices $W \in \R_+^{m \times r}$ and $H \in \R_+^{r \times n}$ that minimize 
a given cost function $D(V, WH)$, that is:
\begin{equation}
\min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} D(V, WH)
\end{equation}
where $D$ is a distance measure between matrices.
\end{definition}
\section{Metrics}
Some introduction here ...
\subsection{Frobenius norm}
Probably the most common choice for the distance measure $D$ in the NMF problem \ref{NMF optimization problem} is the squared Frobenius norm:
\begin{equation}
    D(V, WH) = ||V - WH||_F^2 = \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
This choice leads to the following optimization problem:
\begin{equation}
    \min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} ||V - WH||_F^2
\end{equation}
A justification for the use of Frobenius norm as objective function of the optimization problem \ref{NMF optimization problem} can be given from a statistical point of view.
The Frobenius norm can be obtained using the principle of maximum likelihood estimation assuming that the elements of the matrix $V$ are affected by a i.i.d. Gaussian noise.
See \cite{gillis2020nonnegative} chapter $5.1$.
\par
Suppose that each element of $V$ is generated as:
\begin{equation}
    V[i,j] = (WH)[i,j] + \epsilon_{i,j}
\end{equation}
where $\epsilon_{i,j} \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. Gaussian noise variables with zero mean and variance $\sigma^2$.
The likelihood of observing the matrix $V$ given the parameters $W$ and $H$ is:
\begin{equation}
    L(WH ; V) = \prod_{i=1}^{m} \prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(V[i,j] - (WH)[i,j])^2}{2 \sigma^2}\right)
\end{equation}
The log-likelihood $\ell(WH ; V) = \log L(WH ; V)$  is:
\begin{equation}
    \ell(WH ; V) =  -\frac{mn}{2} \log(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
Maximizing the log-likelihood 
\begin{equation}
    \max_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} \ell(WH ; V)
\end{equation}
is equivalent to minimizing the sum of squared errors:
\begin{equation}
    \min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
which is exactly the Frobenius norm.
\par
\subsection{Kullback-Leibler}
TODO
\subsection{$\beta$-divergence}
TODO
\section{Complexity of NMF}
The main result was proved by Vavasis in \cite{vavasis2007complexitynonnegativematrixfactorization} where it is shown that the NMF problem is NP-hard in general.
To do so Vavasis considers the exact NMF problem, a particular case of NMF where the distance measure is such that $D(V, WH) = 0$ if and only if $V = WH$ and $D(V, WH) > 0$ otherwise.
\begin{definition}[Exact Non-Negative Matrix Factorization problem]\label{Exact NMF definition}
      Given a nonnegative matrix $V \in \R^{m\times n}_+$ and a factorization rank $r$,
compute, if possible, two nonnegative matrices
$W \in \R^{m \times r}_+$ and $H \in \R^{r \times n}_+$
such that
\begin{equation}
    V = WH
\end{equation}
The pair $(W , H)$ is said to be an Exact NMF of $V$ of size $r$.  
\end{definition}
Behind every approximation $\tilde{V}\approx WH$ an exact NMF is hidden.
Exact NMF has a nice \textit{geometric interpretation} which allows one to
characterize the set of solutions.   
\par
Since the NMF problem as stated in \ref{NMF optimization problem} is a generalization of the exact NMF problem, the NP-hardness of exact NMF implies the NP-hardness of NMF.
\section{Identifiability}
Let's start with an observation: In the case of the exact factorization $V = WH$, if we consider any invertible matrix $Q \in \R^{r \times r}$ such that both $WQ$ and $Q^{-1}H$ are non-negative matrices, then we have another exact factorization of $V$ given by:
\begin{equation}
    V = WH = (WQ)(Q^{-1}H)
\end{equation}
This simple observation shows that in general the NMF problem does not have a unique solution.
\par
Even after removing this scaling ambiguity, one should not expect to find an unique NMF solution in general.
in the literature this is also known as the \textit{identifiability} problem of NMF. See \cite{gillis2020nonnegative} chapter $4$ for a detailed discussion on this topic.
\par
In this section we will observe a necessary condition for identifiability and a sufficient condition for identifiability.
To do so we first need the definition of the \textit{support} of a vector.
\begin{definition}[Support of a vector]
Given a vector $x \in \R^n$, the support of $x$ is defined as:
\begin{equation}
    \text{supp}(x) = \{ i \in \{1, \ldots, n\} | x_i \neq 0 \}
\end{equation}
\end{definition}
The next theorem states that a necessary condition for identifiability is that the support of any column of W does not contain the support of any other column of W
\begin{theorem}
[Necessary condition for identifiability]\label{Necessary condition for identifiability}
Let $V \in \R_+^{m \times n}$ be a non-negative matrix and let $V = WH$ be an exact NMF of $V$ of size $r$.
If the exact NMF of $V$ is unique (up to scaling and permutation) then for any pair of distinct columns $W[:,i]$ and $W[:,j]$ of $W$ it holds that:
\begin{equation}
    \text{supp}(W[:,i]) \nsubseteq \text{supp}(W[:,j])
\end{equation} 
\end{theorem}
\begin{proof}
Assume by contradiction that there exist two distinct columns $W[:,i]$ and $W[:,j]$ of $W$ such that:
\begin{equation}
    \text{supp}(W[:,i]) \subseteq \text{supp}(W[:,j])
\end{equation}
Then we can construct a new pair of matrices $(\tilde{W}, \tilde{H})$ as follows:
\begin{equation}
    \tilde{W}[:,k] = 
    \begin{cases}
        W[:,k] & \text{if } k \neq j \\
        W[:,j] + \alpha W[:,i] & \text{if } k = j
    \end{cases}
\end{equation}
\begin{equation}
    \tilde{H}[k,:] = 
    \begin{cases}
        H[k,:] & \text{if } k \neq i \\
        H[i,:] - \alpha H[j,:] & \text{if } k = i
    \end{cases}
\end{equation} 
such that $W[:,j] + \alpha W[:,i] \geq 0$ and $H[i,:] - \alpha H[j,:] \geq 0$ for some $\alpha > 0$.
It is easy to see that $(\tilde{W}, \tilde{H})$ is another exact NMF of $V$ of size $r$:
\begin{equation}
    \tilde{W} \tilde{H} = W H = V
\end{equation}
This contradicts the assumption that the exact NMF of $V$ is unique.
\end{proof}
Now we present a sufficient condition for identifiability known as the \textit{separability} condition.
\begin{theorem}
[Sufficient condition for identifiability: Separability]\label{Sufficient condition for identifiability}
Let $V \in \R_+^{m \times n}$ be a non-negative matrix and let $V = WH$ be an exact NMF of $V$ of size $r$.
The matrices $W$ and $H$ contain non-singular diagonal submatrices of size $r \times r$, then the NMF solutions are identifiable.
\end{theorem}
\begin{proof}
    The properties of separable matrices are analyzed in \cite{donoho2003does}.
\end{proof}
An equivalent definition of separability is the following:
\begin{definition}[Separable NMF]
An NMF $V = WH$ is said to be separable if there exists a set of indices $\{j_1, j_2, \ldots, j_r\} \subseteq \{1, 2, \ldots, n\}$ of size $r$ such that:
\begin{equation}
    V = W H = V[:, \{j_1, j_2, \ldots, j_r\}]\tilde{H}
\end{equation}
\end{definition}
The separable matrices represent an important class of matrices in applications \cite{arora2012computing}, \cite{gillis2013fast}; for example in text mining \cite{arora2013practical}, 
hyperspectral imaging \cite{boardman1995mapping}, and for some computer vision tasks like background subtraction \cite{kumar2015near}.
\section{Geometric Interpretation}
IN QUESTA SEZIONE SI DA UN'INTUIZIONE GEOMETRICA DELL'NMF.
\section{Historical origin of NMF}
Even though some early works date back to the 60s in the field of Earth science and remote system sensing \cite{Wallace1960}\cite{Imbrie1964}\cite{craig1994}
the first modern definition of the NMF problem is usually attributed to the work of Paatero and Tapper in 1994 \cite{Paatero1994} where they defined the so-called Positive Matrix Factorization (PMF) problem, a particular case of NMF where the distance measure used is the least squares one (Frobenius norm).
This early work arose in the field of analytical chemistry and the NMF model as a modern data analysis tool remained relatively unknown until the seminal paper of Lee and Seung in 1999 \cite{Lee1999}.
\par
In their work Lee and Seung introduce the NMF model to the machine learning community and proposed an easy to implement algorithm based on multiplicative updates to find local minima of the NMF optimization problem \ref{NMF optimization problem}.
Their work is especially famous because they  demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces, in contrast to other decomposition methods like PCA or SVD that learn holistic features.
Their results are also reproduced in this thesis in section \ref{Example feature extraction NMF} to show a prominent example of NMF's application.
\section{Two applications of NMF}
In this section two applications of NMF are presented: the first one is the well-known example of learning parts of faces introduced by Lee and Seung in \cite{Lee1999},
while the second one is an application of NMF to text mining, in particular to topic modeling.
\subsection{Learning parts of faces} \label{Example feature extraction NMF}
The dataset used in this example is the CBCL face dataset \footnote{See \ref{code documentation appendix} for data availability and details on the code used}.
This dataset has size $2429 \times 361$ where each column represent a $19 \times 19$ gray-scale image of a face.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Imgs/CBCL_Original.png}
    \caption{Some example images from the CBCL face dataset.}
    \label{fig:CBCL original faces}
\end{figure}
The NMF model is applied to this dataset with a factorization rank of $r = 49$,  the algorithm used is the multiplicative updates with the Kullback-Leibler divergence as distance measure.
After fitting the model we obtain two matrices: $W \in \R_+^{2429 \times 49}$ and $H \in \R_+^{49 \times 361}$.
The non-negativity constraints of the NMF problem \ref{NMF optimization problem} allows one to interpret the columns of $W$ in the same way as the original data, that is each column of $W$ can be reshaped to form a $19 \times 19$ gray-scale image and displayed.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Imgs/CBCL_colums_of_W.png}
    \caption{Columns of the matrix $W$ reshaped as $19 \times 19$ gray-scale images.}
    \label{fig:CBCL columns of W}
\end{figure}
As can be seen in figure \ref{fig:CBCL columns of W} the pixels of the images formed by the columns of $W$ are active only in certain regions, for example some columns represent eyes, others noses, mouths and so on.
This is the reason why it is said that Non-negative matrix factorization is able to learn a ``part based'' representation of faces.
\par
The matrix $H$ contains the coefficients or weights to reconstruct the original images as linear combinations of the parts.\\
The $j$-th column of the original data matrix $V$ can be approximated as:
\begin{equation}
    V[:,j] \approx W H[:,j] = \sum_{k=1}^{r} H[k,j] W[:, k]
\end{equation}
This is a linear combination of the columns of $W$ weighted by the coefficients in the $j$-th column of $H$.
\begin{equation}
    \begin{bmatrix}
        v_{1,j} \\
        v_{2,j} \\
        \vdots \\
        v_{m,j}
    \end{bmatrix} \approx H[1,j]
    \begin{bmatrix}
        w_{1,1} \\
        w_{2,1} \\
        \vdots \\
        w_{m,1}
    \end{bmatrix} + H[2,j]
    \begin{bmatrix}
        w_{1,2} \\
        w_{2,2} \\
        \vdots \\
        w_{m,2}
    \end{bmatrix} + \ldots + H[r,j]
    \begin{bmatrix}
        w_{1,r} \\
        w_{2,r} \\
        \vdots \\
        w_{m,r}
    \end{bmatrix}
\end{equation}
Note that the number of columns of $V$ is equal to the number of columns of $H$. That means that there is a one-to-one correspondence between the columns of $V$ and the columns of $H$ containing the weights to reconstruct each image.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Imgs/Reconstruction_1.png}
    \caption{Example of reconstruction of an image from the CBCL dataset using the NMF model. On the left the original image, on the right the reconstructed one.}
    \label{fig:CBCL reconstructed faces}
\end{figure}
\subsection{Topic modeling}
TODO