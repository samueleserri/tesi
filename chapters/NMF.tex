\chapter{Non-Negative Matrix Factorization}
This chapter introduces the NMF problem.
\section{Definition}
In its more general form the Non-negative matrix factorization problem can be defined as follows: given a non-negative matrix $V \in \R_+^{m \times n}$ and a factorization rank $r \in \N$ find two matrices
$W \in \R_+^{m \times r}$ and $H \in \R_+^{r \times n}$, constrained to be non-negative, such that their product $WH$ approximates $V$ as closely as possible according to some distance measure $D$.
\par
This problem can be formulated with an optimization problem as follows:
\begin{definition}[Non-Negative Matrix Factorization optimization problem]\label{NMF optimization problem}
Given a non-negative matrix $V \in \R_+^{m \times n}$ and a factorization rank $r \in \N$, find two non-negative matrices $W \in \R_+^{m \times r}$ and $H \in \R_+^{r \times n}$ that minimize 
a given cost function $D(V, WH)$, that is:
\begin{equation}
\min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} D(V, WH)
\end{equation}
where $D$ is a distance measure between matrices.
\end{definition}
\section{Metrics}
Some introduction here ...
\subsection{Frobenius norm}
Probably the most used choice for the distance measure $D$ in the NMF problem is the squared Frobenius norm:
\begin{equation}
    D(V, WH) = ||V - WH||_F^2 = \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
This choice leads to the following optimization problem:
\begin{equation}
    \min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} ||V - WH||_F^2
\end{equation}
A justification for the use of Frobenius norm as objective function of the optimization problem can be given from a statistical point of view.
The Frobenius norm can be obtained using the principle of maximum likelihood estimation assuming that the elements of the matrix $V$ are affected by a i.i.d. Gaussian noise.
See \cite{gillis2020nonnegative} chapter $5.1$.
\par
Suppose that each element of $V$ is generated as:
\begin{equation}
    V[i,j] = (WH)[i,j] + \epsilon_{i,j}
\end{equation}
where $\epsilon_{i,j} \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. Gaussian noise variables with zero mean and variance $\sigma^2$.
The likelihood of observing the matrix $V$ given the parameters $W$ and $H$ is:
\begin{equation}
    L(WH ; V) = \prod_{i=1}^{m} \prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(V[i,j] - (WH)[i,j])^2}{2 \sigma^2}\right)
\end{equation}
The log-likelihood $\ell(WH ; V) = \log L(WH ; V)$  is:
\begin{equation}
    \ell(WH ; V) =  -\frac{mn}{2} \log(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
Maximizing the log-likelihood 
\begin{equation}
    \max_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} \ell(WH ; V)
\end{equation}
is equivalent to minimizing the sum of squared errors:
\begin{equation}
    \min_{W \in \R_+^{m \times r}, H \in \R_+^{r \times n}} \sum_{i=1}^{m} \sum_{j=1}^{n} (V[i,j] - (WH)[i,j])^2
\end{equation}
which is exactly the Frobenius norm.
\par
\subsection{Kullback-Leibler}
TODO
\subsection{$\beta$-divergence}
TODO
\section{Complexity of NMF}
The main result was proved by Vavasis in \cite{vavasis2007complexitynonnegativematrixfactorization} where it is shown that the NMF problem is NP-hard in general.
To do so Vavasis considers the exact NMF problem, a particular case of NMF where the distance measure is such that $D(V, WH) = 0$ if and only if $V = WH$ and $D(V, WH) > 0$ otherwise.
\begin{definition}[Exact Non-Negative Matrix Factorization problem]\label{Exact NMF definition}
      Given a nonnegative matrix $V \in \R^{m\times n}_+$ and a factorization rank $r$,
compute, if possible, two nonnegative matrices
$W \in \R^{m \times r}_+$ and $H \in \R^{r \times n}_+$
such that
\begin{equation}
    V = WH
\end{equation}
The pair $(W , H)$ is said to be an Exact NMF of $V$ of size $r$.  
\end{definition}
Behind every approximation $\tilde{V}\approx WH$ an exact NMF is hidden.
Exact NMF has a nice \textit{geometric interpretation} which allows one to
characterize the set of solutions.   
\par
Since the NMF problem as stated in \ref{NMF optimization problem} is a generalization of the exact NMF problem, the NP-hardness of exact NMF implies the NP-hardness of NMF.
\section{Identifiability}
TODO
\section{Geometric Interpretation}
TODO
\section{Historical origin of NMF}
Even though some early works date back to the 70s in the field of Earth science and remote system sensing \cite{Wallace1960}\cite{Imbrie1964}\cite{craig1994}
the first modern definition of the NMF problem is usually attributed to the work of Paatero and Tapper in 1994 \cite{Paatero1994} where they defined the so-called Positive Matrix Factorization (PMF) problem, a particular case of NMF where the distance measure used is the least squares one (Frobenius norm).
This early work arose in the field of analytical chemistry and the NMF model as a modern data analysis tool remained relatively unknown until the seminal paper of Lee and Seung in 1999 \cite{Lee1999}.
\par
In this work Lee and Seung introduces the NMF model to the machine learning community and proposed an easy to implement algorithm based on multiplicative updates to solve the NMF optimization problem.
Their work is especially famous because they  demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces, in contrast to other decomposition methods like PCA or SVD that learn holistic features.
Their results are also reproduced in this thesis to show an example of application of NMF.
\section{Two applications of NMF}
In this section two applications of NMF are presented: the first one is the well-known example of learning parts of faces introduced by Lee and Seung in \cite{Lee1999},
while the second one is an application of NMF to text mining, in particular to topic modeling.
\subsection{Learning parts of faces}
The dataset used in this example is the CBCL face dataset \footnote{See \ref{code documentation appendix} for data availability and details on the code used}.
This dataset has size $2429 \times 361$ where each column represent a $19 \times 19$ gray-scale image of a face.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Imgs/CBCL_Original.png}
    \caption{Some example images from the CBCL face dataset.}
    \label{fig:CBCL original faces}
\end{figure}
The NMF model is applied to this dataset with a factorization rank of $r = 49$,  the algorithm used is the multiplicative updates with the Kullback-Leibler divergence as distance measure.
After fitting the model we obtain two matrices: $W \in \R_+^{2429 \times 49}$ and $H \in \R_+^{49 \times 361}$.
The non-negativity constraints of the NMF problem \ref{NMF optimization problem} allows one to interpret the columns of $W$ in the same way as the original data, that is each column of $W$ can be reshaped to form a $19 \times 19$ gray-scale image and displayed.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Imgs/CBCL_colums_of_W.png}
    \caption{Columns of the matrix $W$ reshaped as $19 \times 19$ gray-scale images.}
    \label{fig:CBCL columns of W}
\end{figure}
As can be seen in figure \ref{fig:CBCL columns of W} the pixels of the images formed by the columns of $W$ are active only in certain regions, for example some columns represent eyes, others noses, mouths and so on.
This is the reason why it is said that Non-negative matrix factorization is able to learn a ``part based'' representation of faces.
\par
The matrix $H$ contains the coefficients or weights to reconstruct the original images as linear combinations of the parts.\\
The $j$-th column of the original data matrix $V$ can be approximated as:
\begin{equation}
    V[:,j] \approx W H[:,j] = \sum_{k=1}^{r} H[k,j] W[:, k]
\end{equation}
This is a linear combination of the columns of $W$ weighted by the coefficients in the $j$-th column of $H$.
\begin{equation}
    \begin{bmatrix}
        v_{1,j} \\
        v_{2,j} \\
        \vdots \\
        v_{m,j}
    \end{bmatrix} \approx H[1,j]
    \begin{bmatrix}
        w_{1,1} \\
        w_{2,1} \\
        \vdots \\
        w_{m,1}
    \end{bmatrix} + H[2,j]
    \begin{bmatrix}
        w_{1,2} \\
        w_{2,2} \\
        \vdots \\
        w_{m,2}
    \end{bmatrix} + \ldots + H[r,j]
    \begin{bmatrix}
        w_{1,r} \\
        w_{2,r} \\
        \vdots \\
        w_{m,r}
    \end{bmatrix}
\end{equation}
Note that the number of columns of $V$ is equal to the number of columns of $H$. That means that there is a one-to-one correspondence between the columns of $V$ and the columns of $H$ containing the weights to reconstruct each image.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Imgs/Reconstruction_1.png}
    \caption{Example of reconstruction of an image from the CBCL dataset using the NMF model. On the left the original image, on the right the reconstructed one.}
    \label{fig:CBCL reconstructed faces}
\end{figure}
\subsection{Topic modeling}
TODO